<!DOCTYPE html><HTML lang="en"><head><meta charset="UTF-8"/><meta content="width=device-width, initial-scale=1.0" name="viewport"/><title>Bayesian Neural ODEs: NUTS · DiffEqFlux.jl</title><link href="https://diffeqflux.sciml.ai/stable/examples/BayesianNODE_NUTS/" rel="canonical"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script data-main="../../assets/documenter.js" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link class="docs-theme-link" data-theme-name="documenter-dark" href="../../assets/themes/documenter-dark.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" data-theme-name="documenter-light" data-theme-primary="" href="../../assets/themes/documenter-light.css" rel="stylesheet" type="text/css"/><script src="../../assets/themeswap.js"></script><script data-outdated-warner="">function maybeAddWarning () {
    const head = document.getElementsByTagName('head')[0];

    // Add a noindex meta tag (unless one exists) so that search engines don't index this version of the docs.
    if (document.body.querySelector('meta[name="robots"]') === null) {
        const meta = document.createElement('meta');
        meta.name = 'robots';
        meta.content = 'noindex';

        head.appendChild(meta);
    };

    // Add a stylesheet to avoid inline styling
    const style = document.createElement('style');
    style.type = 'text/css';
    style.appendChild(document.createTextNode('.outdated-warning-overlay {  position: fixed;  top: 0;  left: 0;  right: 0;  box-shadow: 0 0 10px rgba(0, 0, 0, 0.3);  z-index: 999;  background-color: #ffaba7;  color: rgba(0, 0, 0, 0.7);  border-bottom: 3px solid #da0b00;  padding: 10px 35px;  text-align: center;  font-size: 15px; }  .outdated-warning-overlay .outdated-warning-closer {    position: absolute;    top: calc(50% - 10px);    right: 18px;    cursor: pointer;    width: 12px; }  .outdated-warning-overlay a {    color: #2e63b8; }    .outdated-warning-overlay a:hover {      color: #363636; }'));
    head.appendChild(style);

    const div = document.createElement('div');
    div.classList.add('outdated-warning-overlay');
    const closer = document.createElement('div');
    closer.classList.add('outdated-warning-closer');

    // Icon by font-awesome (license: https://fontawesome.com/license, link: https://fontawesome.com/icons/times?style=solid)
    closer.innerHTML = '<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="times" class="svg-inline--fa fa-times fa-w-11" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 352 512"><path fill="currentColor" d="M242.72 256l100.07-100.07c12.28-12.28 12.28-32.19 0-44.48l-22.24-22.24c-12.28-12.28-32.19-12.28-44.48 0L176 189.28 75.93 89.21c-12.28-12.28-32.19-12.28-44.48 0L9.21 111.45c-12.28 12.28-12.28 32.19 0 44.48L109.28 256 9.21 356.07c-12.28 12.28-12.28 32.19 0 44.48l22.24 22.24c12.28 12.28 32.2 12.28 44.48 0L176 322.72l100.07 100.07c12.28 12.28 32.2 12.28 44.48 0l22.24-22.24c12.28-12.28 12.28-32.19 0-44.48L242.72 256z"></path></svg>';
    closer.addEventListener('click', function () {
        document.body.removeChild(div);
    });
    let href = '/stable';
    if (window.documenterBaseURL) {
        href = window.documenterBaseURL + '/../stable';
    }
    div.innerHTML = 'This is an old version of the documentation. <br> <a href="' + href + '">Go to the newest version</a>.';
    div.appendChild(closer);
    document.body.appendChild(div);
};

if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', maybeAddWarning);
} else {
    maybeAddWarning();
};
</script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img alt="DiffEqFlux.jl logo" src="../../assets/logo.png"/></a><div class="docs-package-name"><span class="docs-autofit">DiffEqFlux.jl</span></div><form action="../../search/" class="docs-search"><input class="docs-search-query" id="documenter-search-query" name="q" placeholder="Search docs" type="text"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)</a></li><li><span class="tocitem">Basic Parameter Fitting Tutorials</span><ul><li><a class="tocitem" href="../optimization_ode/">Optimization of Ordinary Differential Equations</a></li><li><a class="tocitem" href="../optimization_sde/">Optimization of Stochastic Differential Equations</a></li><li><a class="tocitem" href="../lotka_volterra/">Lotka-Volterra with Flux.train!</a></li><li><a class="tocitem" href="../delay_diffeq/">Delay Differential Equations</a></li><li><a class="tocitem" href="../pde_constrained/">Partial Differential Equation Constrained Optimization</a></li><li><a class="tocitem" href="../stiff_ode_fit/">Parameter Estimation on Highly Stiff Systems</a></li></ul></li><li><span class="tocitem">Neural ODE and SDE Tutorials</span><ul><li><a class="tocitem" href="../neural_ode_sciml/">Neural Ordinary Differential Equations with sciml_train</a></li><li><a class="tocitem" href="../neural_ode_flux/">Neural Ordinary Differential Equations with Flux.train!</a></li><li><a class="tocitem" href="../mnist_neural_ode/">GPU-based MNIST Neural ODE Classifier</a></li><li><a class="tocitem" href="../neural_sde/">Neural Stochastic Differential Equations</a></li><li><a class="tocitem" href="../augmented_neural_ode/">Augmented Neural Ordinary Differential Equations</a></li><li><a class="tocitem" href="../collocation/">Smoothed Collocation for Fast Two-Stage Training</a></li><li><a class="tocitem" href="../neural_gde/">Neural Graph Differential Equations</a></li><li><a class="tocitem" href="../normalizing_flows/">Continuous Normalizing Flows with sciml_train</a></li></ul></li><li><span class="tocitem">Bayesian Estimation Tutorials</span><ul><li><a class="tocitem" href="../turing_bayesian/">Bayesian Estimation of Differential Equations with Probabilistic Programming</a></li><li class="is-active"><a class="tocitem" href="">Bayesian Neural ODEs: NUTS</a><ul class="internal"><li><a class="tocitem" href="#Copy-Pasteable-Code-1"><span>Copy-Pasteable Code</span></a></li><li><a class="tocitem" href="#Explanation-1"><span>Explanation</span></a></li></ul></li><li><a class="tocitem" href="../BayesianNODE_SGLD/">Bayesian Neural ODEs: SGLD</a></li></ul></li><li><span class="tocitem">FAQ, Tips, and Tricks</span><ul><li><a class="tocitem" href="../local_minima/">Strategies to Avoid Local Minima</a></li><li><a class="tocitem" href="../multiple_nn/">Simultaneous Fitting of Multiple Neural Networks</a></li><li><a class="tocitem" href="../data_parallel/">Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching</a></li><li><a class="tocitem" href="../second_order_neural/">Neural Second Order Ordinary Differential Equation</a></li><li><a class="tocitem" href="../second_order_adjoints/">Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis</a></li><li><a class="tocitem" href="../minibatch/">Training a Neural Ordinary Differential Equation with Mini-Batching</a></li></ul></li><li><span class="tocitem">Hybrid and Jump Tutorials</span><ul><li><a class="tocitem" href="../hybrid_diffeq/">Training Neural Networks in Hybrid Differential Equations</a></li><li><a class="tocitem" href="../bouncing_ball/">Bouncing Ball Hybrid ODE Optimization</a></li><li><a class="tocitem" href="../jump/">Neural Jump Diffusions (Neural Jump SDE) and Neural Partial Differential Equations (Neural PDEs)</a></li></ul></li><li><span class="tocitem">Optimal and Model Predictive Control Tutorials</span><ul><li><a class="tocitem" href="../optimal_control/">Solving Optimal Control Problems with Universal Differential Equations</a></li><li><a class="tocitem" href="../feedback_control/">Universal Differential Equations for Neural Feedback Control</a></li></ul></li><li><span class="tocitem">Universal Differential Equations and Physical Constraints Tutorials</span><ul><li><a class="tocitem" href="../universal_diffeq/">Universal Ordinary, Stochastic, and Partial Diffrential Equation Examples</a></li><li><a class="tocitem" href="../exogenous_input/">Handling Exogenous Input Signals</a></li><li><a class="tocitem" href="../physical_constraints/">Enforcing Physical Constraints via Universal Differential-Algebraic Equations</a></li><li><a class="tocitem" href="../tensor_layer/">Physics Informed Machine Learning with TensorLayer</a></li><li><a class="tocitem" href="../hamiltonian_nn/">Hamiltonian Neural Network</a></li></ul></li><li><span class="tocitem">Layer APIs</span><ul><li><a class="tocitem" href="../../layers/BasisLayers/">Classical Basis Layers</a></li><li><a class="tocitem" href="../../layers/TensorLayer/">Tensor Product Layer</a></li><li><a class="tocitem" href="../../layers/CNFLayer/">Continuous Normalizing Flows Layer</a></li><li><a class="tocitem" href="../../layers/SplineLayer/">Spline Layer</a></li><li><a class="tocitem" href="../../layers/NeuralDELayers/">Neural Differential Equation Layers</a></li><li><a class="tocitem" href="../../layers/HamiltonianNN/">Hamiltonian Neural Network Layer</a></li></ul></li><li><span class="tocitem">Manual and APIs</span><ul><li><a class="tocitem" href="../../ControllingAdjoints/">Controlling Choices of Adjoints</a></li><li><a class="tocitem" href="../../Flux/">Use with Flux Chain and train!</a></li><li><a class="tocitem" href="../../FastChain/">FastChain</a></li><li><a class="tocitem" href="../../Collocation/">Smoothed Collocation</a></li><li><a class="tocitem" href="../../GPUs/">GPUs</a></li><li><a class="tocitem" href="../../Scimltrain/">sciml_train</a></li></ul></li><li><a class="tocitem" href="../../Benchmark/">Benchmarks</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Bayesian Estimation Tutorials</a></li><li class="is-active"><a href="">Bayesian Neural ODEs: NUTS</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="">Bayesian Neural ODEs: NUTS</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SciML/DiffEqFlux.jl/blob/master/docs/src/examples/BayesianNODE_NUTS.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" href="#" id="documenter-settings-button" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" href="#" id="documenter-sidebar-button"></a></div></header><article class="content" id="documenter-page"><h1 id="Bayesian-Neural-ODEs:-NUTS-1"><a class="docs-heading-anchor" href="#Bayesian-Neural-ODEs:-NUTS-1">Bayesian Neural ODEs: NUTS</a><a class="docs-heading-anchor-permalink" href="#Bayesian-Neural-ODEs:-NUTS-1" title="Permalink"></a></h1><p>In this tutorial, we show how the DiffEqFlux.jl library in Julia can be seamlessly combined with Bayesian estimation libraries like AdvancedHMC.jl and Turing.jl. This enables converting Neural ODEs to Bayesian Neural ODEs, which enables us to estimate the error in the Neural ODE estimation and forecasting. In this tutorial, a working example of the Bayesian Neural ODE: NUTS sampler is shown.</p><p>For more details, please refer to <a href="https://arxiv.org/abs/2012.07244">Bayesian Neural Ordinary Differential Equations</a>.</p><h2 id="Copy-Pasteable-Code-1"><a class="docs-heading-anchor" href="#Copy-Pasteable-Code-1">Copy-Pasteable Code</a><a class="docs-heading-anchor-permalink" href="#Copy-Pasteable-Code-1" title="Permalink"></a></h2><p>Before getting to the explanation, here's some code to start with. We will follow wil a full explanation of the definition and training process:</p><pre><code class="language-julia">using DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Plots, AdvancedHMC, MCMCChains
using JLD, StatsPlots

u0 = [2.0; 0.0]
datasize = 40
tspan = (0.0, 1)
tsteps = range(tspan[1], tspan[2], length = datasize)

function trueODEfunc(du, u, p, t)
    true_A = [-0.1 2.0; -2.0 -0.1]
    du .= ((u.^3)'true_A)'
end

prob_trueode = ODEProblem(trueODEfunc, u0, tspan)
ode_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))


dudt2 = FastChain((x, p) -&gt; x.^3,
                  FastDense(2, 50, tanh),
                  FastDense(50, 2))
prob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)

function predict_neuralode(p)
    Array(prob_neuralode(u0, p))
end

function loss_neuralode(p)
    pred = predict_neuralode(p)
    loss = sum(abs2, ode_data .- pred)
    return loss, pred
end

l(θ) = -sum(abs2, ode_data .- predict_neuralode(θ)) - sum(θ .* θ)


function dldθ(θ)
    x,lambda = Flux.Zygote.pullback(l,θ)
    grad = first(lambda(1))
    return x, grad
end

metric  = DiagEuclideanMetric(length(prob_neuralode.p))

h = Hamiltonian(metric, l, dldθ)


integrator = Leapfrog(find_good_stepsize(h, Float64.(prob_neuralode.p)))


prop = AdvancedHMC.NUTS{MultinomialTS, GeneralisedNoUTurn}(integrator)

adaptor = StanHMCAdaptor(MassMatrixAdaptor(metric), StepSizeAdaptor(0.45, prop.integrator))

samples, stats = sample(h, prop, Float64.(prob_neuralode.p), 500, adaptor, 500; progress=true)


losses = map(x-&gt; x[1],[loss_neuralode(samples[i]) for i in 1:length(samples)])

##################### PLOTS: LOSSES ###############
scatter(losses, ylabel = "Loss",  yscale= :log, label = "Architecture1: 500 warmup, 500 sample")

################### RETRODICTED PLOTS: TIME SERIES #################
pl = scatter(tsteps, ode_data[1,:], color = :red, label = "Data: Var1", xlabel = "t", title = "Spiral Neural ODE")
scatter!(tsteps, ode_data[2,:], color = :blue, label = "Data: Var2")

for k in 1:300
    resol = predict_neuralode(samples[100:end][rand(1:400)])
    plot!(tsteps,resol[1,:], alpha=0.04, color = :red, label = "")
    plot!(tsteps,resol[2,:], alpha=0.04, color = :blue, label = "")
end

idx = findmin(losses)[2]
prediction = predict_neuralode(samples[idx])

plot!(tsteps,prediction[1,:], color = :black, w = 2, label = "")
plot!(tsteps,prediction[2,:], color = :black, w = 2, label = "Best fit prediction", ylims = (-2.5, 3.5))



#################### RETRODICTED PLOTS - CONTOUR ####################
pl = scatter(ode_data[1,:], ode_data[2,:], color = :red, label = "Data",  xlabel = "Var1", ylabel = "Var2", title = "Spiral Neural ODE")

for k in 1:300
    resol = predict_neuralode(samples[100:end][rand(1:400)])
    plot!(resol[1,:],resol[2,:], alpha=0.04, color = :red, label = "")
end

plot!(prediction[1,:], prediction[2,:], color = :black, w = 2, label = "Best fit prediction", ylims = (-2.5, 3))
</code></pre><p>Time Series Plots:</p><p><img alt="" src="https://user-images.githubusercontent.com/23134958/102398119-df940a00-4004-11eb-9cdb-eb7be8724dd3.png"/></p><p>Contour Plots:</p><p><img alt="" src="https://user-images.githubusercontent.com/23134958/102398114-defb7380-4004-11eb-835e-84f1519648dc.png"/></p><pre><code class="language-julia">######################## CHAIN DIAGNOSIS PLOTS#########################
samples = hcat(samples...)

samples_reduced = samples[1:5, :]

samples_reshape = reshape(samples_reduced, (500, 5, 1))

Chain_Spiral = Chains(samples_reshape)

plot(Chain_Spiral)

autocorplot(Chain_Spiral)
</code></pre><p>Chain Mixing Plot:</p><p><img alt="" src="https://user-images.githubusercontent.com/23134958/102398106-dd31b000-4004-11eb-9623-a24ab0409b07.png"/></p><p>Auto-Corelation Plot:</p><p><img alt="" src="https://user-images.githubusercontent.com/23134958/102398102-dacf5600-4004-11eb-853a-60faa67422ef.png"/></p><h2 id="Explanation-1"><a class="docs-heading-anchor" href="#Explanation-1">Explanation</a><a class="docs-heading-anchor-permalink" href="#Explanation-1" title="Permalink"></a></h2><h4 id="Step-1:-Get-the-data-from-the-Spiral-ODE-example-1"><a class="docs-heading-anchor" href="#Step-1:-Get-the-data-from-the-Spiral-ODE-example-1">Step 1: Get the data from the Spiral ODE example</a><a class="docs-heading-anchor-permalink" href="#Step-1:-Get-the-data-from-the-Spiral-ODE-example-1" title="Permalink"></a></h4><pre><code class="language-julia">u0 = [2.0; 0.0]
datasize = 40
tspan = (0.0, 1)
tsteps = range(tspan[1], tspan[2], length = datasize)

function trueODEfunc(du, u, p, t)
    true_A = [-0.1 2.0; -2.0 -0.1]
    du .= ((u.^3)'true_A)'
end

prob_trueode = ODEProblem(trueODEfunc, u0, tspan)
ode_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))</code></pre><h4 id="Step-2:-Define-the-Neural-ODE-architecture.-1"><a class="docs-heading-anchor" href="#Step-2:-Define-the-Neural-ODE-architecture.-1">Step 2: Define the Neural ODE architecture.</a><a class="docs-heading-anchor-permalink" href="#Step-2:-Define-the-Neural-ODE-architecture.-1" title="Permalink"></a></h4><p>Note that this step potentially offers a lot of flexibility in the number of layers/ number of units in each layer. It may not necessarily be true that a 100 units architecture is better at prediction/forecasting than a 50 unit architecture. On the other hand, a complicated architecture can take a huge computational time without increasing performance.</p><pre><code class="language-julia">dudt2 = FastChain((x, p) -&gt; x.^3,
                  FastDense(2, 50, tanh),
                  FastDense(50, 2))
prob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)</code></pre><h4 id="Step-3:-Define-the-loss-function-for-the-Neural-ODE.-1"><a class="docs-heading-anchor" href="#Step-3:-Define-the-loss-function-for-the-Neural-ODE.-1">Step 3: Define the loss function for the Neural ODE.</a><a class="docs-heading-anchor-permalink" href="#Step-3:-Define-the-loss-function-for-the-Neural-ODE.-1" title="Permalink"></a></h4><pre><code class="language-julia">function predict_neuralode(p)
    Array(prob_neuralode(u0, p))
end

function loss_neuralode(p)
    pred = predict_neuralode(p)
    loss = sum(abs2, ode_data .- pred)
    return loss, pred
end
</code></pre><h4 id="Step-4:-Now-we-start-integrating-the-Bayesian-estimation-workflow-as-prescribed-by-the-AdvancedHMC-interface-with-the-NeuralODE-defined-above.-1"><a class="docs-heading-anchor" href="#Step-4:-Now-we-start-integrating-the-Bayesian-estimation-workflow-as-prescribed-by-the-AdvancedHMC-interface-with-the-NeuralODE-defined-above.-1">Step 4: Now we start integrating the Bayesian estimation workflow as prescribed by the AdvancedHMC interface with the NeuralODE defined above.</a><a class="docs-heading-anchor-permalink" href="#Step-4:-Now-we-start-integrating-the-Bayesian-estimation-workflow-as-prescribed-by-the-AdvancedHMC-interface-with-the-NeuralODE-defined-above.-1" title="Permalink"></a></h4><p>The Advanced HMC interface requires us to specify: (a) the hamiltonian log density and its gradient , (b) the sampler and (c) the step size adaptor function.</p><p>For the hamiltonian log density, we use the loss function. The θ*θ term denotes the use of Gaussian priors.</p><p>The user can make several modifications to Step 4. The user can try different acceptance ratios, warmup samples and posterior samples. One can also use the Variational Inference (ADVI) framework, which doesn't work quite as well as NUTS. The SGLD (Stochastic Langevin Gradient Descent) sampler is seen to have a better performance than NUTS. Have a look at https://sebastiancallh.github.io/post/langevin/ for a quick introduction to SGLD.</p><pre><code class="language-julia">l(θ) = -sum(abs2, ode_data .- predict_neuralode(θ)) - sum(θ .* θ)


function dldθ(θ)
    x,lambda = Flux.Zygote.pullback(l,θ)
    grad = first(lambda(1))
    return x, grad
end

metric  = DiagEuclideanMetric(length(prob_neuralode.p))

h = Hamiltonian(metric, l, dldθ)
</code></pre><p>We use the NUTS sampler with a acceptance ratio of δ= 0.45 in this example. In addition, we use Nesterov Dual Averaging for the Step Size adaptation.</p><p>We sample using 500 warmup samples and 500 posterior samples.</p><pre><code class="language-julia">
integrator = Leapfrog(find_good_stepsize(h, Float64.(prob_neuralode.p)))


prop = AdvancedHMC.NUTS{MultinomialTS, GeneralisedNoUTurn}(integrator)

adaptor = StanHMCAdaptor(MassMatrixAdaptor(metric), StepSizeAdaptor(0.45, prop.integrator))

samples, stats = sample(h, prop, Float64.(prob_neuralode.p), 500, adaptor, 500; progress=true)
</code></pre><h4 id="Step-5:-Plot-diagnostics.-1"><a class="docs-heading-anchor" href="#Step-5:-Plot-diagnostics.-1">Step 5: Plot diagnostics.</a><a class="docs-heading-anchor-permalink" href="#Step-5:-Plot-diagnostics.-1" title="Permalink"></a></h4><p>A: Plot chain object and auto-corel plot of the first 5 parameters.</p><pre><code class="language-julia">samples = hcat(samples...)

samples_reduced = samples[1:5, :]

samples_reshape = reshape(samples_reduced, (500, 5, 1))

Chain_Spiral = Chains(samples_reshape)

plot(Chain_Spiral)

autocorplot(Chain_Spiral)</code></pre><p>B: Plot retrodicted data.</p><pre><code class="language-julia">
####################TIME SERIES PLOTS###################
pl = scatter(tsteps, ode_data[1,:], color = :red, label = "Data: Var1", xlabel = "t", title = "Spiral Neural ODE")
scatter!(tsteps, ode_data[2,:], color = :blue, label = "Data: Var2")

for k in 1:300
    resol = predict_neuralode(samples[100:end][rand(1:400)])
    plot!(tsteps,resol[1,:], alpha=0.04, color = :red, label = "")
    plot!(tsteps,resol[2,:], alpha=0.04, color = :blue, label = "")
end

idx = findmin(losses)[2]
prediction = predict_neuralode(samples[idx])

plot!(tsteps,prediction[1,:], color = :black, w = 2, label = "")
plot!(tsteps,prediction[2,:], color = :black, w = 2, label = "Best fit prediction", ylims = (-2.5, 3.5))

####################CONTOUR PLOTS#########################3
pl = scatter(ode_data[1,:], ode_data[2,:], color = :red, label = "Data",  xlabel = "Var1", ylabel = "Var2", title = "Spiral Neural ODE")

for k in 1:300
    resol = predict_neuralode(samples[100:end][rand(1:400)])
    plot!(resol[1,:],resol[2,:], alpha=0.04, color = :red, label = "")
end

plot!(prediction[1,:], prediction[2,:], color = :black, w = 2, label = "Best fit prediction", ylims = (-2.5, 3))
</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../turing_bayesian/">« Bayesian Estimation of Differential Equations with Probabilistic Programming</a><a class="docs-footer-nextpage" href="../BayesianNODE_SGLD/">Bayesian Neural ODEs: SGLD »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label></p><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div><p></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 15 February 2021 12:10">Monday 15 February 2021</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></HTML>