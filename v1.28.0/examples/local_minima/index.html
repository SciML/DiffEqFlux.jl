<!DOCTYPE html><HTML lang="en"><head><meta charset="UTF-8"/><meta content="width=device-width, initial-scale=1.0" name="viewport"/><title>Strategies to Avoid Local Minima · DiffEqFlux.jl</title><link href="https://diffeqflux.sciml.ai/stable/examples/local_minima/" rel="canonical"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script data-main="../../assets/documenter.js" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link class="docs-theme-link" data-theme-name="documenter-dark" href="../../assets/themes/documenter-dark.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" data-theme-name="documenter-light" data-theme-primary="" href="../../assets/themes/documenter-light.css" rel="stylesheet" type="text/css"/><script src="../../assets/themeswap.js"></script><script data-outdated-warner="">function maybeAddWarning () {
    const head = document.getElementsByTagName('head')[0];

    // Add a noindex meta tag (unless one exists) so that search engines don't index this version of the docs.
    if (document.body.querySelector('meta[name="robots"]') === null) {
        const meta = document.createElement('meta');
        meta.name = 'robots';
        meta.content = 'noindex';

        head.appendChild(meta);
    };

    // Add a stylesheet to avoid inline styling
    const style = document.createElement('style');
    style.type = 'text/css';
    style.appendChild(document.createTextNode('.outdated-warning-overlay {  position: fixed;  top: 0;  left: 0;  right: 0;  box-shadow: 0 0 10px rgba(0, 0, 0, 0.3);  z-index: 999;  background-color: #ffaba7;  color: rgba(0, 0, 0, 0.7);  border-bottom: 3px solid #da0b00;  padding: 10px 35px;  text-align: center;  font-size: 15px; }  .outdated-warning-overlay .outdated-warning-closer {    position: absolute;    top: calc(50% - 10px);    right: 18px;    cursor: pointer;    width: 12px; }  .outdated-warning-overlay a {    color: #2e63b8; }    .outdated-warning-overlay a:hover {      color: #363636; }'));
    head.appendChild(style);

    const div = document.createElement('div');
    div.classList.add('outdated-warning-overlay');
    const closer = document.createElement('div');
    closer.classList.add('outdated-warning-closer');

    // Icon by font-awesome (license: https://fontawesome.com/license, link: https://fontawesome.com/icons/times?style=solid)
    closer.innerHTML = '<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="times" class="svg-inline--fa fa-times fa-w-11" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 352 512"><path fill="currentColor" d="M242.72 256l100.07-100.07c12.28-12.28 12.28-32.19 0-44.48l-22.24-22.24c-12.28-12.28-32.19-12.28-44.48 0L176 189.28 75.93 89.21c-12.28-12.28-32.19-12.28-44.48 0L9.21 111.45c-12.28 12.28-12.28 32.19 0 44.48L109.28 256 9.21 356.07c-12.28 12.28-12.28 32.19 0 44.48l22.24 22.24c12.28 12.28 32.2 12.28 44.48 0L176 322.72l100.07 100.07c12.28 12.28 32.2 12.28 44.48 0l22.24-22.24c12.28-12.28 12.28-32.19 0-44.48L242.72 256z"></path></svg>';
    closer.addEventListener('click', function () {
        document.body.removeChild(div);
    });
    let href = '/stable';
    if (window.documenterBaseURL) {
        href = window.documenterBaseURL + '/../stable';
    }
    div.innerHTML = 'This is an old version of the documentation. <br> <a href="' + href + '">Go to the newest version</a>.';
    div.appendChild(closer);
    document.body.appendChild(div);
};

if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', maybeAddWarning);
} else {
    maybeAddWarning();
};
</script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img alt="DiffEqFlux.jl logo" src="../../assets/logo.png"/></a><div class="docs-package-name"><span class="docs-autofit">DiffEqFlux.jl</span></div><form action="../../search/" class="docs-search"><input class="docs-search-query" id="documenter-search-query" name="q" placeholder="Search docs" type="text"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)</a></li><li><span class="tocitem">Basic Parameter Fitting Tutorials</span><ul><li><a class="tocitem" href="../optimization_ode/">Optimization of Ordinary Differential Equations</a></li><li><a class="tocitem" href="../optimization_sde/">Optimization of Stochastic Differential Equations</a></li><li><a class="tocitem" href="../lotka_volterra/">Lotka-Volterra with Flux.train!</a></li><li><a class="tocitem" href="../delay_diffeq/">Delay Differential Equations</a></li><li><a class="tocitem" href="../pde_constrained/">Partial Differential Equation Constrained Optimization</a></li></ul></li><li><span class="tocitem">Neural ODE and SDE Tutorials</span><ul><li><a class="tocitem" href="../neural_ode_sciml/">Neural Ordinary Differential Equations with sciml_train</a></li><li><a class="tocitem" href="../neural_ode_flux/">Neural Ordinary Differential Equations with Flux.train!</a></li><li><a class="tocitem" href="../mnist_neural_ode/">GPU-based MNIST Neural ODE Classifier</a></li><li><a class="tocitem" href="../neural_sde/">Neural Stochastic Differential Equations</a></li><li><a class="tocitem" href="../augmented_neural_ode/">Augmented Neural Ordinary Differential Equations</a></li><li><a class="tocitem" href="../collocation/">Smoothed Collocation for Fast Two-Stage Training</a></li><li><a class="tocitem" href="../neural_gde/">Neural Graph Differential Equations</a></li><li><a class="tocitem" href="../normalizing_flows/">Continuous Normalizing Flows with sciml_train</a></li></ul></li><li><span class="tocitem">Bayesian Estimation Tutorials</span><ul><li><a class="tocitem" href="../turing_bayesian/">Bayesian Estimation of Differential Equations with Probabilistic Programming</a></li><li><a class="tocitem" href="../BayesianNODE_NUTS/">Bayesian Neural ODEs: NUTS</a></li><li><a class="tocitem" href="../BayesianNODE_SGLD/">Bayesian Neural ODEs: SGLD</a></li></ul></li><li><span class="tocitem">FAQ, Tips, and Tricks</span><ul><li class="is-active"><a class="tocitem" href="">Strategies to Avoid Local Minima</a><ul class="internal"><li><a class="tocitem" href="#allow_f_increasestrue-1"><span><code>allow_f_increases=true</code></span></a></li><li><a class="tocitem" href="#Iterative-Growing-Of-Fits-to-Reduce-Probability-of-Bad-Local-Minima-1"><span>Iterative Growing Of Fits to Reduce Probability of Bad Local Minima</span></a></li><li><a class="tocitem" href="#Training-both-the-initial-conditions-and-the-parameters-to-start-1"><span>Training both the initial conditions and the parameters to start</span></a></li></ul></li><li><a class="tocitem" href="../multiple_nn/">Simultaneous Fitting of Multiple Neural Networks</a></li><li><a class="tocitem" href="../data_parallel/">Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching</a></li><li><a class="tocitem" href="../second_order_neural/">Neural Second Order Ordinary Differential Equation</a></li><li><a class="tocitem" href="../second_order_adjoints/">Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis</a></li><li><a class="tocitem" href="../minibatch/">Training a Neural Ordinary Differential Equation with Mini-Batching</a></li></ul></li><li><span class="tocitem">Hybrid and Jump Tutorials</span><ul><li><a class="tocitem" href="../hybrid_diffeq/">Training Neural Networks in Hybrid Differential Equations</a></li><li><a class="tocitem" href="../jump/">Neural Jump Diffusions (Neural Jump SDE) and Neural Partial Differential Equations (Neural PDEs)</a></li></ul></li><li><span class="tocitem">Optimal and Model Predictive Control Tutorials</span><ul><li><a class="tocitem" href="../optimal_control/">Solving Optimal Control Problems with Universal Differential Equations</a></li><li><a class="tocitem" href="../feedback_control/">Universal Differential Equations for Neural Feedback Control</a></li></ul></li><li><span class="tocitem">Universal Differential Equations and Physical Constraints Tutorials</span><ul><li><a class="tocitem" href="../universal_diffeq/">Universal Ordinary, Stochastic, and Partial Diffrential Equation Examples</a></li><li><a class="tocitem" href="../exogenous_input/">Handling Exogenous Input Signals</a></li><li><a class="tocitem" href="../physical_constraints/">Enforcing Physical Constraints via Universal Differential-Algebraic Equations</a></li><li><a class="tocitem" href="../tensor_layer/">Physics Informed Machine Learning with TensorLayer</a></li><li><a class="tocitem" href="../hamiltonian_nn/">Hamiltonian Neural Network</a></li></ul></li><li><span class="tocitem">Layers</span><ul><li><a class="tocitem" href="../../layers/BasisLayers/">Classical Basis Layers</a></li><li><a class="tocitem" href="../../layers/TensorLayer/">Tensor Product Layer</a></li><li><a class="tocitem" href="../../layers/CNFLayer/">Continuous Normalizing Flows Layer</a></li><li><a class="tocitem" href="../../layers/SplineLayer/">Spline Layer</a></li><li><a class="tocitem" href="../../layers/NeuralDELayers/">Neural Differential Equation Layers</a></li><li><a class="tocitem" href="../../layers/HamiltonianNN/">Hamiltonian Neural Network Layer</a></li></ul></li><li><a class="tocitem" href="../../ControllingAdjoints/">Controlling Choices of Adjoints</a></li><li><a class="tocitem" href="../../Flux/">Use with Flux Chain and train!</a></li><li><a class="tocitem" href="../../FastChain/">FastChain</a></li><li><a class="tocitem" href="../../Collocation/">Smoothed Collocation</a></li><li><a class="tocitem" href="../../GPUs/">GPUs</a></li><li><a class="tocitem" href="../../Scimltrain/">sciml_train</a></li><li><a class="tocitem" href="../../Benchmark/">Benchmark</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">FAQ, Tips, and Tricks</a></li><li class="is-active"><a href="">Strategies to Avoid Local Minima</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="">Strategies to Avoid Local Minima</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SciML/DiffEqFlux.jl/blob/master/docs/src/examples/local_minima.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" href="#" id="documenter-settings-button" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" href="#" id="documenter-sidebar-button"></a></div></header><article class="content" id="documenter-page"><h1 id="Strategies-to-Avoid-Local-Minima-1"><a class="docs-heading-anchor" href="#Strategies-to-Avoid-Local-Minima-1">Strategies to Avoid Local Minima</a><a class="docs-heading-anchor-permalink" href="#Strategies-to-Avoid-Local-Minima-1" title="Permalink"></a></h1><p>Local minima can be an issue with fitting neural differential equations. However, there are many strategies to avoid local minima:</p><ol><li>Insert stochasticity into the loss function through minibatching</li><li>Weigh the loss function to allow for fitting earlier portions first</li><li>Changing the optimizers to <code>allow_f_increases</code></li><li>Iteratively grow the fit</li><li>Training the initial conditions and the parmeters to start</li></ol><h2 id="allow_f_increasestrue-1"><a class="docs-heading-anchor" href="#allow_f_increasestrue-1"><code>allow_f_increases=true</code></a><a class="docs-heading-anchor-permalink" href="#allow_f_increasestrue-1" title="Permalink"></a></h2><p>With Optim.jl optimizers, you can set <code>allow_f_increases=true</code> in order to let increases in the loss function not cause an automatic halt of the optimization process. Using a method like BFGS or NewtonTrustRegion is not guaranteed to have monotonic convergence and so this can stop early exits which can result in local minima. This looks like:</p><pre><code class="language-julia">pmin = DiffEqFlux.sciml_train(loss_neuralode, pstart, NewtonTrustRegion(), cb=cb,
                              maxiters = 200, allow_f_increases = true)</code></pre><h2 id="Iterative-Growing-Of-Fits-to-Reduce-Probability-of-Bad-Local-Minima-1"><a class="docs-heading-anchor" href="#Iterative-Growing-Of-Fits-to-Reduce-Probability-of-Bad-Local-Minima-1">Iterative Growing Of Fits to Reduce Probability of Bad Local Minima</a><a class="docs-heading-anchor-permalink" href="#Iterative-Growing-Of-Fits-to-Reduce-Probability-of-Bad-Local-Minima-1" title="Permalink"></a></h2><p>In this example we will show how to use strategy (4) in order to increase the robustness of the fit. Let's start with the same neural ODE example we've used before except with one small twist: we wish to find the neural ODE that fits on <code>(0,5.0)</code>. Naively, we use the same training strategy as before:</p><pre><code class="language-julia">using DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Plots

u0 = Float32[2.0; 0.0]
datasize = 30
tspan = (0.0f0, 5.0f0)
tsteps = range(tspan[1], tspan[2], length = datasize)

function trueODEfunc(du, u, p, t)
    true_A = [-0.1 2.0; -2.0 -0.1]
    du .= ((u.^3)'true_A)'
end

prob_trueode = ODEProblem(trueODEfunc, u0, tspan)
ode_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))

dudt2 = FastChain((x, p) -&gt; x.^3,
                  FastDense(2, 16, tanh),
                  FastDense(16, 2))
prob_neuralode = NeuralODE(dudt2, tspan, Vern7(), saveat = tsteps, abstol=1e-6, reltol=1e-6)

function predict_neuralode(p)
  Array(prob_neuralode(u0, p))
end

function loss_neuralode(p)
    pred = predict_neuralode(p)
    loss = sum(abs2, (ode_data[:,1:size(pred,2)] .- pred))
    return loss, pred
end

iter = 0
callback = function (p, l, pred; doplot = true)
  global iter
  iter += 1

  display(l)
  if doplot
    # plot current prediction against data
    plt = scatter(tsteps[1:size(pred,2)], ode_data[1,1:size(pred,2)], label = "data")
    scatter!(plt, tsteps[1:size(pred,2)], pred[1,:], label = "prediction")
    display(plot(plt))
  end

  return false
end

result_neuralode = DiffEqFlux.sciml_train(loss_neuralode, prob_neuralode.p,
                                          ADAM(0.05), cb = callback,
                                          maxiters = 300)

callback(result_neuralode2.minimizer,loss_neuralode(result_neuralode.minimizer)...;doplot=true)
savefig("local_minima.png")</code></pre><p><img alt="" src="https://user-images.githubusercontent.com/1814174/81901710-f82ed400-958c-11ea-993f-118f5513d170.png"/></p><p>However, we've now fallen into a trap of a local minima. If the optimizer changes the parameters so it dips early, it will increase the loss because there will be more error in the later parts of the time series. Thus it tends to just stay flat and never fit perfectly. This thus suggests strategies (2) and (3): do not allow the later parts of the time series to influence the fit until the later stages. Strategy (3) seems to be more robust, so this is what will be demonstrated.</p><p>Let's start by reducing the timespan to <code>(0,1.5)</code>:</p><pre><code class="language-julia">prob_neuralode = NeuralODE(dudt2, (0.0,1.5), Tsit5(), saveat = tsteps[tsteps .&lt;= 1.5])

result_neuralode2 = DiffEqFlux.sciml_train(loss_neuralode, prob_neuralode.p,
                                           ADAM(0.05), cb = callback,
                                           maxiters = 300)

callback(result_neuralode2.minimizer,loss_neuralode(result_neuralode2.minimizer)...;doplot=true)
savefig("shortplot1.png")</code></pre><p><img alt="" src="https://user-images.githubusercontent.com/1814174/81901707-f82ed400-958c-11ea-9e8e-0efb10d9b05c.png"/></p><p>This fits beautifully. Now let's grow the timespan and utilize the parameters from our <code>(0,1.5)</code> fit as the initial condition to our next fit:</p><pre><code class="language-julia">prob_neuralode = NeuralODE(dudt2, (0.0,3.0), Tsit5(), saveat = tsteps[tsteps .&lt;= 3.0])

result_neuralode3 = DiffEqFlux.sciml_train(loss_neuralode,
                                           result_neuralode2.minimizer,
                                           ADAM(0.05), maxiters = 300,
                                           cb = callback)
callback(result_neuralode3.minimizer,loss_neuralode(result_neuralode3.minimizer)...;doplot=true)
savefig("shortplot2.png")</code></pre><p><img alt="" src="https://user-images.githubusercontent.com/1814174/81901706-f7963d80-958c-11ea-856a-7f85af8695b8.png"/></p><p>Once again a great fit. Now we utilize these parameters as the initial condition to the full fit:</p><pre><code class="language-julia">prob_neuralode = NeuralODE(dudt2, (0.0,5.0), Tsit5(), saveat = tsteps)

result_neuralode4 = DiffEqFlux.sciml_train(loss_neuralode,
                                           result_neuralode3.minimizer,
                                           ADAM(0.01), maxiters = 300,
                                           cb = callback)
callback(result_neuralode4.minimizer,loss_neuralode(result_neuralode4.minimizer)...;doplot=true)
savefig("fullplot.png")</code></pre><p><img alt="" src="https://user-images.githubusercontent.com/1814174/81901711-f82ed400-958c-11ea-9ba2-2b1f213b865a.png"/></p><h2 id="Training-both-the-initial-conditions-and-the-parameters-to-start-1"><a class="docs-heading-anchor" href="#Training-both-the-initial-conditions-and-the-parameters-to-start-1">Training both the initial conditions and the parameters to start</a><a class="docs-heading-anchor-permalink" href="#Training-both-the-initial-conditions-and-the-parameters-to-start-1" title="Permalink"></a></h2><p>In this example we will show how to use strategy (5) in order to accomplish the same goal, except rather than growing the trajectory iteratively, we can train on the whole trajectory. We do this by allowing the neural ODE to learn both the  initial conditions and parameters to start, and then reset the initial conditions back and train only the parameters. Note: this strategy is demonstrated for the (0, 5) time span and (0, 10), any longer and more iterations will be required. Alternatively, one could use a mix of (4) and (5), or breaking up the trajectory into chunks and just (5).</p><pre><code class="language-julia">
using DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Plots, DifferentialEquations


#Starting example with tspan (0, 5)
u0 = Float32[2.0; 0.0]
datasize = 30
tspan = (0.0f0, 5.0f0)
tsteps = range(tspan[1], tspan[2], length = datasize)

function trueODEfunc(du, u, p, t)
    true_A = [-0.1 2.0; -2.0 -0.1]
    du .= ((u.^3)'true_A)'
end

prob_trueode = ODEProblem(trueODEfunc, u0, tspan)
ode_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))

#Using flux here to easily demonstrate the idea, but this can be done with sciml_train!
dudt2 = Chain(Dense(2,16, tanh),
             Dense(16,2))


p,re = Flux.destructure(dudt2) # use this p as the initial condition!
dudt(u,p,t) = re(p)(u) # need to restrcture for backprop!
prob = ODEProblem(dudt,u0,tspan)

function predict_n_ode()
    Array(solve(prob,u0=u0,p=p, saveat=tsteps))
end

function loss_n_ode()
      pred = predict_n_ode()
      sqnorm(x) = sum(abs2, x)
      loss = sum(abs2,ode_data .- pred)
      loss
end

function cb(;doplot=false) #callback function to observe training
    pred = predict_n_ode()
    display(sum(abs2,ode_data .- pred))
    if doplot
      # plot current prediction against data
      pl = plot(tsteps,ode_data[1,:],label="data")
      plot!(pl,tsteps,pred[1,:],label="prediction")
      display(plot(pl))
    end
    return false
end
predict_n_ode()
loss_n_ode()
cb(;doplot=true)

data = Iterators.repeated((), 1000)

#Specify to flux to include both the initial conditions (IC) and parameters of the NODE to train
Flux.train!(loss_n_ode, Flux.params(u0, p), data,
                    Flux.Optimise.ADAM(0.05), cb = cb)

#Here we reset the IC back to the original and train only the NODE parameters
u0 = Float32[2.0; 0.0]
Flux.train!(loss_n_ode, Flux.params(p), data,
            Flux.Optimise.ADAM(0.05), cb = cb)

cb(;doplot=true)

#Now use the same technique for a longer tspan (0, 10)
datasize = 30
tspan = (0.0f0, 10.0f0)
tsteps = range(tspan[1], tspan[2], length = datasize)

prob_trueode = ODEProblem(trueODEfunc, u0, tspan)
ode_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))

dudt2 = Chain(Dense(2,16, tanh),
             Dense(16,2))

p,re = Flux.destructure(dudt2) # use this p as the initial condition!
dudt(u,p,t) = re(p)(u) # need to restrcture for backprop!
prob = ODEProblem(dudt,u0,tspan)



data = Iterators.repeated((), 1500)

Flux.train!(loss_n_ode, Flux.params(u0, p), data,
                    Flux.Optimise.ADAM(0.05), cb = cb)



u0 = Float32[2.0; 0.0]
Flux.train!(loss_n_ode, Flux.params(p), data,
            Flux.Optimise.ADAM(0.05), cb = cb)

cb(;doplot=true)
</code></pre><p>And there we go, a set of robust strategies for fitting an equation that would otherwise get stuck in a local optima.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../BayesianNODE_SGLD/">« Bayesian Neural ODEs: SGLD</a><a class="docs-footer-nextpage" href="../multiple_nn/">Simultaneous Fitting of Multiple Neural Networks »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label></p><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div><p></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 21 December 2020 13:58">Monday 21 December 2020</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></HTML>