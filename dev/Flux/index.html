<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Use with Flux Chain and train! · DiffEqFlux.jl</title><link rel="canonical" href="https://diffeqflux.sciml.ai/stable/Flux/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="DiffEqFlux.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">DiffEqFlux.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../examples/optimization_ode/">Optimization of Ordinary Differential Equations</a></li><li><a class="tocitem" href="../examples/optimization_sde/">Optimization of Stochastic Differential Equations</a></li><li><a class="tocitem" href="../examples/lotka_volterra/">Lotka-Volterra with Flux.train!</a></li><li><a class="tocitem" href="../examples/neural_ode_sciml/">Neural Ordinary Differential Equations with sciml_train</a></li><li><a class="tocitem" href="../examples/neural_ode_flux/">Neural Ordinary Differential Equations with Flux.train!</a></li><li><a class="tocitem" href="../examples/mnist_neural_ode/">GPU-based MNIST Neural ODE Classifier</a></li><li><a class="tocitem" href="../examples/delay_diffeq/">Delay Differential Equations</a></li><li><a class="tocitem" href="../examples/hybrid_diffeq/">Training Neural Networks in Hybrid Differential Equations</a></li><li><a class="tocitem" href="../examples/augmented_neural_ode/">Augmented Neural Ordinary Differential Equations</a></li><li><a class="tocitem" href="../examples/normalizing_flows/">Continuous Normalizing Flows with sciml_train</a></li><li><a class="tocitem" href="../examples/local_minima/">Strategies to Avoid Local Minima</a></li><li><a class="tocitem" href="../examples/neural_sde/">Neural Stochastic Differential Equations</a></li><li><a class="tocitem" href="../examples/collocation/">Smoothed Collocation for Fast Two-Stage Training</a></li><li><a class="tocitem" href="../examples/exogenous_input/">Handling Exogenous Input Signals</a></li><li><a class="tocitem" href="../examples/pde_constrained/">Partial Differential Equation Constrained Optimization</a></li><li><a class="tocitem" href="../examples/optimal_control/">Solving Optimal Control Problems with Universal Differential Equations (@id optcontrol)</a></li><li><a class="tocitem" href="../examples/feedback_control/">Universal Differential Equations for Neural Feedback Control</a></li><li><a class="tocitem" href="../examples/second_order_neural/">Neural Second Order Ordinary Differential Equation</a></li><li><a class="tocitem" href="../examples/physical_constraints/">Enforcing Physical Constraints via Universal Differential-Algebraic Equations</a></li><li><a class="tocitem" href="../examples/second_order_adjoints/">Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis</a></li><li><a class="tocitem" href="../examples/jump/">Neural Jump Diffusions (Neural Jump SDE) and Neural Partial Differential Equations (Neural PDEs)</a></li><li><a class="tocitem" href="../examples/universal_diffeq/">Universal Ordinary, Stochastic, and Partial Diffrential Equation Examples</a></li><li><a class="tocitem" href="../examples/minibatch/">Training a Neural Ordinary Differential Equation with Mini-Batching</a></li><li><a class="tocitem" href="../examples/tensor_layer/">Physics Informed Machine Learning with TensorLayer</a></li><li><a class="tocitem" href="../examples/neural_gde/">Neural Graph Differential Equations</a></li><li><a class="tocitem" href="../examples/hamiltonian_nn/">Hamiltonian Neural Network</a></li></ul></li><li><span class="tocitem">Layers</span><ul><li><a class="tocitem" href="../layers/BasisLayers/">Classical Basis Layers</a></li><li><a class="tocitem" href="../layers/TensorLayer/">Tensor Product Layer</a></li><li><a class="tocitem" href="../layers/CNFLayer/">Continuous Normalizing Flows Layer</a></li><li><a class="tocitem" href="../layers/SplineLayer/">Spline Layer</a></li><li><a class="tocitem" href="../layers/NeuralDELayers/">Neural Differential Equation Layers</a></li><li><a class="tocitem" href="../layers/HamiltonianNN/">Hamiltonian Neural Network Layer</a></li></ul></li><li><a class="tocitem" href="../ControllingAdjoints/">Controlling Choices of Adjoints</a></li><li class="is-active"><a class="tocitem" href>Use with Flux Chain and train!</a><ul class="internal"><li><a class="tocitem" href="#Using-Flux-Chain-neural-networks-with-Flux.train!-1"><span>Using Flux <code>Chain</code> neural networks with Flux.train!</span></a></li><li><a class="tocitem" href="#Using-Flux-Chain-neural-networks-with-sciml_train-1"><span>Using Flux <code>Chain</code> neural networks with sciml_train</span></a></li><li><a class="tocitem" href="#Using-ComponentArrays-for-neural-network-layers-1"><span>Using ComponentArrays for neural network layers</span></a></li></ul></li><li><a class="tocitem" href="../FastChain/">FastChain</a></li><li><a class="tocitem" href="../Collocation/">Smoothed Collocation</a></li><li><a class="tocitem" href="../GPUs/">GPUs</a></li><li><a class="tocitem" href="../Scimltrain/">sciml_train</a></li><li><a class="tocitem" href="../Benchmark/">Benchmark</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Use with Flux Chain and train!</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Use with Flux Chain and train!</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SciML/DiffEqFlux.jl/blob/master/docs/src/Flux.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Use-with-Flux.jl-1"><a class="docs-heading-anchor" href="#Use-with-Flux.jl-1">Use with Flux.jl</a><a class="docs-heading-anchor-permalink" href="#Use-with-Flux.jl-1" title="Permalink"></a></h1><p>All of the tools of DiffEqFlux.jl can be used with Flux.jl. A lot of the examples have been written to use <code>FastChain</code> and <code>sciml_train</code>, but in all cases this can be changed to the <code>Chain</code> and <code>Flux.train!</code> workflow.</p><h2 id="Using-Flux-Chain-neural-networks-with-Flux.train!-1"><a class="docs-heading-anchor" href="#Using-Flux-Chain-neural-networks-with-Flux.train!-1">Using Flux <code>Chain</code> neural networks with Flux.train!</a><a class="docs-heading-anchor-permalink" href="#Using-Flux-Chain-neural-networks-with-Flux.train!-1" title="Permalink"></a></h2><p>This should work almost automatically by using <code>solve</code>. Here is an example of optimizing <code>u0</code> and <code>p</code>.</p><pre><code class="language-julia">using DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Plots

u0 = Float32[2.; 0.]
datasize = 30
tspan = (0.0f0,1.5f0)

function trueODEfunc(du,u,p,t)
    true_A = [-0.1 2.0; -2.0 -0.1]
    du .= ((u.^3)&#39;true_A)&#39;
end
t = range(tspan[1],tspan[2],length=datasize)
prob = ODEProblem(trueODEfunc,u0,tspan)
ode_data = Array(solve(prob,Tsit5(),saveat=t))

dudt2 = Chain(x -&gt; x.^3,
             Dense(2,50,tanh),
             Dense(50,2))
dudt(u,p,t) = dudt2(u)
prob = ODEProblem(dudt,u0,tspan)

function predict_n_ode()
  Array(solve(prob,Tsit5(),u0=u0,p=p,saveat=t))
end

function loss_n_ode()
    pred = predict_n_ode()
    loss = sum(abs2,ode_data .- pred)
    loss
end

loss_n_ode() # n_ode.p stores the initial parameters of the neural ODE

cb = function (;doplot=false) #callback function to observe training
  pred = predict_n_ode()
  display(sum(abs2,ode_data .- pred))
  # plot current prediction against data
  pl = scatter(t,ode_data[1,:],label=&quot;data&quot;)
  scatter!(pl,t,pred[1,:],label=&quot;prediction&quot;)
  display(plot(pl))
  return false
end

# Display the ODE with the initial parameter values.
cb()

data = Iterators.repeated((), 1000)
res1 = Flux.train!(loss_n_ode, Flux.params(u0,p), data, ADAM(0.05), cb = cb)</code></pre><h2 id="Using-Flux-Chain-neural-networks-with-sciml_train-1"><a class="docs-heading-anchor" href="#Using-Flux-Chain-neural-networks-with-sciml_train-1">Using Flux <code>Chain</code> neural networks with sciml_train</a><a class="docs-heading-anchor-permalink" href="#Using-Flux-Chain-neural-networks-with-sciml_train-1" title="Permalink"></a></h2><p>While for simple neural networks we recommend using <code>FastChain</code>-based neural networks for speed and simplicity, Flux neural networks can be used with <code>sciml_train</code> by utilizing the <code>Flux.destructure</code> function. In this case, if <code>dudt</code> is a Flux chain, then:</p><pre><code class="language-julia">p,re = Flux.destructure(chain)</code></pre><p>returns <code>p</code> which is the vector of parameters for the chain and <code>re</code> which is a function <code>re(p)</code> that reconstructs the neural network with new parameters <code>p</code>. Using this function we can thus build our neural differential equations in an explicit parameter style. For example, the neural ordinary differential equation example written out without using the <code>NeuralODE</code> helper would look like. Notice that in this example we will optimize both the neural network parameters <code>p</code> and the input initial condition <code>u0</code>. Notice that <code>sciml_train</code> works on a vector input, so we have to concatenate <code>u0</code> and <code>p</code> and then in the loss function split to the pieces.</p><pre><code class="language-julia">using DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Plots

u0 = Float32[2.; 0.]
datasize = 30
tspan = (0.0f0,1.5f0)

function trueODEfunc(du,u,p,t)
    true_A = [-0.1 2.0; -2.0 -0.1]
    du .= ((u.^3)&#39;true_A)&#39;
end
t = range(tspan[1],tspan[2],length=datasize)
prob = ODEProblem(trueODEfunc,u0,tspan)
ode_data = Array(solve(prob,Tsit5(),saveat=t))

dudt2 = Chain(x -&gt; x.^3,
             Dense(2,50,tanh),
             Dense(50,2))
p,re = Flux.destructure(dudt2) # use this p as the initial condition!
dudt(u,p,t) = re(p)(u) # need to restrcture for backprop!
prob = ODEProblem(dudt,u0,tspan)

θ = [u0;p] # the parameter vector to optimize

function predict_n_ode(θ)
  Array(solve(prob,Tsit5(),u0=θ[1:2],p=θ[3:end],saveat=t))
end

function loss_n_ode(θ)
    pred = predict_n_ode(θ)
    loss = sum(abs2,ode_data .- pred)
    loss,pred
end

loss_n_ode(θ)

cb = function (θ,l,pred;doplot=false) #callback function to observe training
  display(l)
  # plot current prediction against data
  pl = scatter(t,ode_data[1,:],label=&quot;data&quot;)
  scatter!(pl,t,pred[1,:],label=&quot;prediction&quot;)
  display(plot(pl))
  return false
end

# Display the ODE with the initial parameter values.
cb(θ,loss_n_ode(θ)...)

data = Iterators.repeated((), 1000)
res1 = DiffEqFlux.sciml_train(loss_n_ode, θ, ADAM(0.05), cb = cb, maxiters=100)
cb(res1.minimizer,loss_n_ode(res1.minimizer)...;doplot=true)
res2 = DiffEqFlux.sciml_train(loss_n_ode, res1.minimizer, LBFGS(), cb = cb)
cb(res2.minimizer,loss_n_ode(res2.minimizer)...;doplot=true)</code></pre><p>Notice that the advantage of this format is that we can use Optim&#39;s optimizers, like <code>LBFGS</code> with a full <code>Chain</code> object for all of Flux&#39;s neural networks, like convolutional neural networks.</p><h2 id="Using-ComponentArrays-for-neural-network-layers-1"><a class="docs-heading-anchor" href="#Using-ComponentArrays-for-neural-network-layers-1">Using ComponentArrays for neural network layers</a><a class="docs-heading-anchor-permalink" href="#Using-ComponentArrays-for-neural-network-layers-1" title="Permalink"></a></h2><p>We can also create the dense layers from scratch using <a href="https://github.com/jonniedie/ComponentArrays.jl">ComponentArrays.jl</a>. Flux is used here just for the <code>glorot_uniform</code> function and the <code>ADAM</code> optimizer.</p><pre><code class="language-julia">using ComponentArrays, DiffEqFlux, Optim, OrdinaryDiffEq, Plots, UnPack
using Flux: glorot_uniform, ADAM</code></pre><p>Again, let&#39;s create the truth data.</p><pre><code class="language-julia">u0 = Float32[2.; 0.]
datasize = 30
tspan = (0.0f0, 1.5f0)

function trueODEfunc(du, u, p, t)
    true_A = [-0.1 2.0; -2.0 -0.1]
    du .= ((u.^3)&#39;true_A)&#39;
end

t = range(tspan[1], tspan[2], length = datasize)
prob = ODEProblem(trueODEfunc, u0, tspan)
ode_data = Array(solve(prob, Tsit5(), saveat = t))</code></pre><p>Now, we&#39;ll make a function that creates dense neural layer components. It is similar to <code>Flux.Dense</code>, except it doesn&#39;t handle the activation function. We&#39;ll do that separately.</p><pre><code class="language-julia">dense_layer(in, out) = ComponentArray{Float32}(W=glorot_uniform(out, in), b=zeros(out))</code></pre><p>Our parameter vector will be a <code>ComponentArray</code> that holds the ODE initial conditions and the dense neural layers. This enables it to pass through the solver as a flat array while giving us the convenience of struct-like access to the components.</p><pre><code class="language-julia">layers = (L1=dense_layer(2, 50), L2=dense_layer(50, 2))
θ = ComponentArray(u=u0, p=layers)

function dudt(u, p, t)
    @unpack L1, L2 = p
    return L2.W * tanh.(L1.W * u.^3 .+ L1.b) .+ L2.b
end

prob = ODEProblem(dudt, u0, tspan)</code></pre><p>As before, we&#39;ll define prediction and loss functions as well as a callback function to observe training.</p><pre><code class="language-julia">predict_n_ode(θ) = Array(solve(prob, Tsit5(), u0=θ.u, p=θ.p, saveat=t))

function loss_n_ode(θ)
    pred = predict_n_ode(θ)
    loss = sum(abs2, ode_data .- pred)
    return loss, pred
end
loss_n_ode(θ)

cb = function (θ, loss, pred; doplot=false)
    display(loss)
    # plot current prediction against data
    pl = scatter(t, ode_data[1,:], label = &quot;data&quot;)
    scatter!(pl, t, pred[1,:], label = &quot;prediction&quot;)
    display(plot(pl))
    return false
end

cb(θ, loss_n_ode(θ)...)

data = Iterators.repeated((), 1000)

res1 = DiffEqFlux.sciml_train(loss_n_ode, θ, ADAM(0.05); cb=cb, maxiters=100)
cb(res1.minimizer, loss_n_ode(res1.minimizer)...; doplot=true)

res2 = DiffEqFlux.sciml_train(loss_n_ode, res1.minimizer, LBFGS(); cb=cb)
cb(res2.minimizer, loss_n_ode(res2.minimizer)...; doplot=true)</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../ControllingAdjoints/">« Controlling Choices of Adjoints</a><a class="docs-footer-nextpage" href="../FastChain/">FastChain »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 15 December 2020 12:14">Tuesday 15 December 2020</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
