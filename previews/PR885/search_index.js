var documenterSearchIndex = {"docs":
[{"location":"layers/SplineLayer/#Spline-Layer","page":"Spline Layer","title":"Spline Layer","text":"","category":"section"},{"location":"layers/SplineLayer/","page":"Spline Layer","title":"Spline Layer","text":"Constructs a Spline Layer. At a high-level, it performs the following:","category":"page"},{"location":"layers/SplineLayer/","page":"Spline Layer","title":"Spline Layer","text":"Takes as input a one-dimensional training dataset, a time span, a time step and an interpolation method.\nDuring training, adjusts the values of the function at multiples of the time-step such that the curve interpolated through these points has minimum loss on the corresponding one-dimensional dataset.","category":"page"},{"location":"layers/SplineLayer/","page":"Spline Layer","title":"Spline Layer","text":"SplineLayer","category":"page"},{"location":"layers/SplineLayer/#DiffEqFlux.SplineLayer","page":"Spline Layer","title":"DiffEqFlux.SplineLayer","text":"SplineLayer(time_span, time_step, spline_basis, init_saved_points = nothing)\n\nConstructs a Spline Layer. At a high-level, it performs the following:\n\nTakes as input a one-dimensional training dataset, a time span, a time step and an interpolation method.\nDuring training, adjusts the values of the function at multiples of the time-step such that the curve interpolated through these points has minimum loss on the corresponding one-dimensional dataset.\n\nArguments:\n\ntime_span: Tuple of real numbers corresponding to the time span.\ntime_step: Real number corresponding to the time step.\nspline_basis: Interpolation method to be used yb the basis (current supported interpolation methods: ConstantInterpolation, LinearInterpolation, QuadraticInterpolation, QuadraticSpline, CubicSpline).\n'initsavedpoints': values of the function at multiples of the time step. Initialized by default to a random vector sampled from the unit normal. Alternatively, can take a function with the signature init_saved_points(rng, time_span, time_step).\n\n\n\n\n\n","category":"type"},{"location":"layers/CNFLayer/#CNF-Layer-Functions","page":"Continuous Normalizing Flows Layer","title":"CNF Layer Functions","text":"","category":"section"},{"location":"layers/CNFLayer/","page":"Continuous Normalizing Flows Layer","title":"Continuous Normalizing Flows Layer","text":"The following layers are helper functions for easily building neural differential equation architectures specialized for the task of density estimation through Continuous Normalizing Flows (CNF).","category":"page"},{"location":"layers/CNFLayer/","page":"Continuous Normalizing Flows Layer","title":"Continuous Normalizing Flows Layer","text":"FFJORD\nFFJORDDistribution","category":"page"},{"location":"layers/CNFLayer/#DiffEqFlux.FFJORD","page":"Continuous Normalizing Flows Layer","title":"DiffEqFlux.FFJORD","text":"FFJORD(model, tspan, input_dims, args...; ad = AutoForwardDiff(),\n    basedist = nothing, kwargs...)\n\nConstructs a continuous-time recurrent neural network, also known as a neural ordinary differential equation (neural ODE), with fast gradient calculation via adjoints [1] and specialized for density estimation based on continuous normalizing flows (CNF) [2] with a stochastic approach [2] for the computation of the trace of the dynamics' jacobian. At a high level this corresponds to the following steps:\n\nParameterize the variable of interest x(t) as a function f(z, θ, t) of a base variable z(t) with known density p_z;\nUse the transformation of variables formula to predict the density px as a function of the density pz and the trace of the Jacobian of f;\nChoose the parameter θ to minimize a loss function of p_x (usually the negative likelihood of the data);\n\nAfter these steps one may use the NN model and the learned θ to predict the density p_x for new values of x.\n\nArguments:\n\nmodel: A Flux.Chain or Lux.AbstractExplicitLayer neural network that defines the dynamics of the model.\nbasedist: Distribution of the base variable. Set to the unit normal by default.\ninput_dims: Input Dimensions of the model.\ntspan: The timespan to be solved on.\nargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\nad: The automatic differentiation method to use for the internal jacobian trace. Defaults to AutoForwardDiff().\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\nReferences:\n\n[1] Pontryagin, Lev Semenovich. Mathematical theory of optimal processes. CRC press, 1987.\n\n[2] Chen, Ricky TQ, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. \"Neural ordinary differential equations.\" In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp. 6572-6583. 2018.\n\n[3] Grathwohl, Will, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. \"Ffjord: Free-form continuous dynamics for scalable reversible generative models.\" arXiv preprint arXiv:1810.01367 (2018).\n\n\n\n\n\n","category":"type"},{"location":"layers/CNFLayer/#DiffEqFlux.FFJORDDistribution","page":"Continuous Normalizing Flows Layer","title":"DiffEqFlux.FFJORDDistribution","text":"FFJORD can be used as a distribution to generate new samples by rand or estimate densities by pdf or logpdf (from Distributions.jl).\n\nArguments:\n\nmodel: A FFJORD instance\nregularize: Whether we use regularization (default: false)\nmonte_carlo: Whether we use monte carlo (default: true)\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#Neural-Differential-Equation-Layer-Functions","page":"Neural Differential Equation Layers","title":"Neural Differential Equation Layer Functions","text":"","category":"section"},{"location":"layers/NeuralDELayers/","page":"Neural Differential Equation Layers","title":"Neural Differential Equation Layers","text":"The following layers are helper functions for easily building neural differential equation architectures in the currently most efficient way. As demonstrated in the tutorials, they do not have to be used since automatic differentiation will just work over solve, but these cover common use cases and choose what's known to be the optimal mode of AD for the respective equation type.","category":"page"},{"location":"layers/NeuralDELayers/","page":"Neural Differential Equation Layers","title":"Neural Differential Equation Layers","text":"NeuralODE\nNeuralDSDE\nNeuralSDE\nNeuralCDDE\nNeuralDAE\nNeuralODEMM\nAugmentedNDELayer","category":"page"},{"location":"layers/NeuralDELayers/#DiffEqFlux.NeuralODE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralODE","text":"NeuralODE(model, tspan, alg = nothing, args...; kwargs...)\n\nConstructs a continuous-time recurrant neural network, also known as a neural ordinary differential equation (neural ODE), with a fast gradient calculation via adjoints [1]. At a high level this corresponds to solving the forward differential equation, using a second differential equation that propagates the derivatives of the loss backwards in time.\n\nArguments:\n\nmodel: A Flux.Chain or Lux.AbstractExplicitLayer neural network that defines the ̇x.\ntspan: The timespan to be solved on.\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorithm used in the backpropogation. Defaults to an adjoint method. See the Local Sensitivity Analysis documentation for more details.\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\nReferences:\n\n[1] Pontryagin, Lev Semenovich. Mathematical theory of optimal processes. CRC press, 1987.\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#DiffEqFlux.NeuralDSDE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralDSDE","text":"NeuralDSDE(drift, diffusion, tspan, alg = nothing, args...; sensealg = TrackerAdjoint(),\n    kwargs...)\n\nConstructs a neural stochastic differential equation (neural SDE) with diagonal noise.\n\nArguments:\n\ndrift: A Flux.Chain or Lux.AbstractExplicitLayer neural network that defines the drift function.\ndiffusion: A Flux.Chain or Lux.AbstractExplicitLayer neural network that defines the diffusion function. Should output a vector of the same size as the input.\ntspan: The timespan to be solved on.\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorithm used in the backpropogation.\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#DiffEqFlux.NeuralSDE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralSDE","text":"NeuralSDE(drift, diffusion, tspan, nbrown, alg = nothing, args...;\n    sensealg=TrackerAdjoint(),kwargs...)\n\nConstructs a neural stochastic differential equation (neural SDE).\n\nArguments:\n\ndrift: A Flux.Chain or Lux.AbstractExplicitLayer neural network that defines the drift function.\ndiffusion: A Flux.Chain or Lux.AbstractExplicitLayer neural network that defines the diffusion function. Should output a matrix that is nbrown x size(x, 1).\ntspan: The timespan to be solved on.\nnbrown: The number of Brownian processes\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorithm used in the backpropogation.\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#DiffEqFlux.NeuralCDDE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralCDDE","text":"NeuralCDDE(model, tspan, hist, lags, alg = nothing, args...;\n    sensealg = TrackerAdjoint(), kwargs...)\n\nConstructs a neural delay differential equation (neural DDE) with constant delays.\n\nArguments:\n\nmodel: A Flux.Chain or Lux.AbstractExplicitLayer neural network that defines the derivative function. Should take an input of size [x; x(t - lag_1); ...; x(t - lag_n)] and produce and output shaped like x.\ntspan: The timespan to be solved on.\nhist: Defines the history function h(u, p, t) for values before the start of the integration. Note that u is supposed to be used to return a value that matches the size of u.\nlags: Defines the lagged values that should be utilized in the neural network.\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorithm used in the backpropogation. Defaults to using reverse-mode automatic differentiation via Tracker.jl\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#DiffEqFlux.NeuralDAE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralDAE","text":"NeuralDAE(model, constraints_model, tspan, args...; differential_vars = nothing,\n    sensealg = TrackerAdjoint(), kwargs...)\n\nConstructs a neural differential-algebraic equation (neural DAE).\n\nArguments:\n\nmodel: A Flux.Chain or Lux.AbstractExplicitLayer neural network that defines the derivative function. Should take an input of size x and produce the residual of f(dx,x,t) for only the differential variables.\nconstraints_model: A function constraints_model(u,p,t) for the fixed constraints to impose on the algebraic equations.\ntspan: The timespan to be solved on.\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorithm used in the backpropogation. Defaults to using reverse-mode automatic differentiation via Tracker.jl\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#DiffEqFlux.NeuralODEMM","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralODEMM","text":"NeuralODEMM(model, constraints_model, tspan, mass_matrix, alg = nothing, args...;\n    sensealg = InterpolatingAdjoint(autojacvec = ZygoteVJP()), kwargs...)\n\nConstructs a physically-constrained continuous-time recurrant neural network, also known as a neural differential-algebraic equation (neural DAE), with a mass matrix and a fast gradient calculation via adjoints [1]. The mass matrix formulation is:\n\nMu = f(upt)\n\nwhere M is semi-explicit, i.e. singular with zeros for rows corresponding to the constraint equations.\n\nArguments:\n\nmodel: A Flux.Chain or Lux.AbstractExplicitLayer neural network that defines the ̇f(u,p,t)\nconstraints_model: A function constraints_model(u,p,t) for the fixed constraints to impose on the algebraic equations.\ntspan: The timespan to be solved on.\nmass_matrix: The mass matrix associated with the DAE\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl. This method requires an implicit ODE solver compatible with singular mass matrices. Consult the DAE solvers documentation for more details.\nsensealg: The choice of differentiation algorithm used in the backpropogation. Defaults to an adjoint method. See the Local Sensitivity Analysis documentation for more details.\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#DiffEqFlux.AugmentedNDELayer","page":"Neural Differential Equation Layers","title":"DiffEqFlux.AugmentedNDELayer","text":"AugmentedNDELayer(nde, adim::Int)\n\nConstructs an Augmented Neural Differential Equation Layer.\n\nArguments:\n\nnde: Any Neural Differential Equation Layer\nadim: The number of dimensions the initial conditions should be lifted\n\nReferences:\n\n[1] Dupont, Emilien, Arnaud Doucet, and Yee Whye Teh. \"Augmented neural ODEs.\" In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pp. 3140-3150. 2019.\n\n\n\n\n\n","category":"function"},{"location":"layers/NeuralDELayers/#Helper-Layer-Functions","page":"Neural Differential Equation Layers","title":"Helper Layer Functions","text":"","category":"section"},{"location":"layers/NeuralDELayers/","page":"Neural Differential Equation Layers","title":"Neural Differential Equation Layers","text":"DimMover","category":"page"},{"location":"layers/NeuralDELayers/#DiffEqFlux.DimMover","page":"Neural Differential Equation Layers","title":"DiffEqFlux.DimMover","text":"DimMover(from, to)\n\nConstructs a Dimension Mover Layer.\n\nWe can have Flux's conventional order (data, channel, batch) by using it as the last layer of Flux.Chain to swap the batch-index and the time-index of the Neural DE's output considering that each time point is a channel.\n\n\n\n\n\n","category":"type"},{"location":"examples/tensor_layer/#Physics-Informed-Machine-Learning-(PIML)-with-TensorLayer","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"","category":"section"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"In this tutorial, we show how to use the DiffEqFlux TensorLayer to solve problems in Physics Informed Machine Learning.","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"Let's consider the anharmonic oscillator described by the ODE","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"x = - kx - αx³ - βx -γx³","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"To obtain the training data, we solve the equation of motion using one of the solvers in DifferentialEquations:","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"using ComponentArrays,\n    DiffEqFlux, Optimization, OptimizationOptimisers,\n    OrdinaryDiffEq, LinearAlgebra, Random\nk, α, β, γ = 1, 0.1, 0.2, 0.3\ntspan = (0.0, 10.0)\n\nfunction dxdt_train(du, u, p, t)\n    du[1] = u[2]\n    du[2] = -k * u[1] - α * u[1]^3 - β * u[2] - γ * u[2]^3\nend\n\nu0 = [1.0, 0.0]\nts = collect(0.0:0.1:tspan[2])\nprob_train = ODEProblem{true}(dxdt_train, u0, tspan)\ndata_train = Array(solve(prob_train, Tsit5(); saveat = ts))","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"Now, we create a TensorLayer that will be able to perform 10th order expansions in a Legendre Basis:","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"A = [LegendreBasis(10), LegendreBasis(10)]\nnn = TensorLayer(A, 1)\nps, st = Lux.setup(Random.default_rng(), nn)\nps = ComponentArray(ps)\nnn = Lux.Experimental.StatefulLuxLayer(nn, nothing, st)","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"and we also instantiate the model we are trying to learn, “informing” the neural about the ∝x and ∝v dependencies in the equation of motion:","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"f = x -> min(30one(x), x)\n\nfunction dxdt_pred(du, u, p, t)\n    du[1] = u[2]\n    du[2] = -p.p_model[1] * u[1] - p.p_model[2] * u[2] + f(nn(u, p.ps)[1])\nend\n\np_model = zeros(2)\nα = ComponentArray(; p_model, ps = ps .* 0)\n\nprob_pred = ODEProblem{true}(dxdt_pred, u0, tspan, α)","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"Note that we introduced a “cap” in the neural network term to avoid instabilities in the solution of the ODE. We also initialized the vector of parameters to zero in order to obtain a faster convergence for this particular example.","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"Finally, we introduce the corresponding loss function:","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"function predict_adjoint(θ)\n    x = Array(solve(prob_pred, Tsit5(); p = θ, saveat = ts,\n        sensealg = InterpolatingAdjoint(; autojacvec = ReverseDiffVJP(true))))\nend\n\nfunction loss_adjoint(θ)\n    x = predict_adjoint(θ)\n    loss = sum(norm.(x - data_train))\n    return loss\nend\n\niter = 0\nfunction callback(θ, l)\n    global iter\n    iter += 1\n    if iter % 10 == 0\n        println(l)\n    end\n    return false\nend","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"and we train the network using two rounds of Adam:","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"adtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x, p) -> loss_adjoint(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, α)\nres1 = Optimization.solve(optprob, Adam(0.05); callback = callback, maxiters = 150)\n\noptprob2 = Optimization.OptimizationProblem(optf, res1.u)\nres2 = Optimization.solve(optprob2, Adam(0.001); callback = callback, maxiters = 150)\nopt = res2.u","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"We plot the results, and we obtain a fairly accurate learned model:","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"using Plots\ndata_pred = predict_adjoint(res1.u)\nplot(ts, data_train[1, :]; label = \"X (ODE)\")\nplot!(ts, data_train[2, :]; label = \"V (ODE)\")\nplot!(ts, data_pred[1, :]; label = \"X (NN)\")\nplot!(ts, data_pred[2, :]; label = \"V (NN)\")","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"(Image: plot_tutorial)","category":"page"},{"location":"utilities/Collocation/#Smoothed-Collocation","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"","category":"section"},{"location":"utilities/Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"Smoothed collocation, also referred to as the two-stage method, allows for fitting differential equations to time series data without relying on a numerical differential equation solver by building a smoothed collocating polynomial and using this to estimate the true (u',u) pairs, at which point u'-f(u,p,t) can be directly estimated as a loss to determine the correct parameters p. This method can be extremely fast and robust to noise, though, because it does not accumulate through time, is not as exact as other methods.","category":"page"},{"location":"utilities/Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"note: Note\nThis is one of many methods for calculating the collocation coefficients for the training process. For a more comprehensive set of collocation methods, see the JuliaSimModelOptimizer.","category":"page"},{"location":"utilities/Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"collocate_data","category":"page"},{"location":"utilities/Collocation/#DiffEqFlux.collocate_data","page":"Smoothed Collocation","title":"DiffEqFlux.collocate_data","text":"u′, u = collocate_data(data, tpoints, kernel = TriangularKernel(), bandwidth=nothing)\nu′, u = collocate_data(data, tpoints, tpoints_sample, interp, args...)\n\nComputes a non-parametrically smoothed estimate of u' and u given the data, where each column is a snapshot of the timeseries at tpoints[i].\n\nFor kernels, the following exist:\n\nEpanechnikovKernel\nUniformKernel\nTriangularKernel\nQuarticKernel\nTriweightKernel\nTricubeKernel\nGaussianKernel\nCosineKernel\nLogisticKernel\nSigmoidKernel\nSilvermanKernel\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC2631937/\n\nAdditionally, we can use interpolation methods from DataInterpolations.jl to generate data from intermediate timesteps. In this case, pass any of the methods like QuadraticInterpolation as interp, and the timestamps to sample from as tpoints_sample.\n\n\n\n\n\n","category":"function"},{"location":"utilities/Collocation/#Kernel-Choice","page":"Smoothed Collocation","title":"Kernel Choice","text":"","category":"section"},{"location":"utilities/Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"Note that the kernel choices of DataInterpolations.jl, such as CubicSpline(), are exact, i.e. go through the data points, while the smoothed kernels are regression splines. Thus CubicSpline() is preferred if the data is not too noisy or is relatively sparse. If data is sparse and very noisy, a BSpline() can be the best regression spline, otherwise one of the other kernels such as as EpanechnikovKernel.","category":"page"},{"location":"utilities/Collocation/#Non-Allocating-Forward-Mode-L2-Collocation-Loss","page":"Smoothed Collocation","title":"Non-Allocating Forward-Mode L2 Collocation Loss","text":"","category":"section"},{"location":"utilities/Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"The following is an example of a loss function over the collocation that is non-allocating and compatible with forward-mode automatic differentiation:","category":"page"},{"location":"utilities/Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"using PreallocationTools\ndu = PreallocationTools.dualcache(similar(prob.u0))\npreview_est_sol = [@view estimated_solution[:, i] for i in 1:size(estimated_solution, 2)]\npreview_est_deriv = [@view estimated_derivative[:, i] for i in 1:size(estimated_solution, 2)]\n\nfunction construct_iip_cost_function(f, du, preview_est_sol, preview_est_deriv, tpoints)\n    function (p)\n        _du = PreallocationTools.get_tmp(du, p)\n        vecdu = vec(_du)\n        cost = zero(first(p))\n        for i in 1:length(preview_est_sol)\n            est_sol = preview_est_sol[i]\n            f(_du, est_sol, p, tpoints[i])\n            vecdu .= vec(preview_est_deriv[i]) .- vec(_du)\n            cost += sum(abs2, vecdu)\n        end\n        sqrt(cost)\n    end\nend\ncost_function = construct_iip_cost_function(f,\n    du,\n    preview_est_sol,\n    preview_est_deriv,\n    tpoints)","category":"page"},{"location":"examples/multiple_shooting/#Multiple-Shooting","page":"Multiple Shooting","title":"Multiple Shooting","text":"","category":"section"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"note: Note\nThe form of multiple shooting found here is a specialized form for implicit layer deep learning (known as data shooting) which assumes full observability of the underlying dynamics and lack of noise. For a more general implementation of multiple shooting, see the JuliaSimModelOptimizer. For an implementation more directly tied to parameter estimation against data, see DiffEqParamEstim.jl.","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"In Multiple Shooting, the training data is split into overlapping intervals. The solver is then trained on individual intervals. If the end conditions of any interval coincide with the initial conditions of the next immediate interval, then the joined/combined solution is the same as solving on the whole dataset (without splitting).","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"To ensure that the overlapping part of two consecutive intervals coincide, we add a penalizing term:","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"continuity_term * absolute_value_of(prediction of last point of group i - prediction of first point of group i+1)","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"to the loss.","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"Note that the continuity_term should have a large positive value to add high penalties in case the solver predicts discontinuous values.","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"The following is a working demo, using Multiple Shooting:","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"using ComponentArrays,\n    Lux, DiffEqFlux, Optimization, OptimizationPolyalgorithms, OrdinaryDiffEq, Plots\nusing DiffEqFlux: group_ranges\n\nusing Random\nrng = Random.default_rng()\n\n# Define initial conditions and time steps\ndatasize = 30\nu0 = Float32[2.0, 0.0]\ntspan = (0.0f0, 5.0f0)\ntsteps = range(tspan[1], tspan[2]; length = datasize)\n\n# Get the data\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u .^ 3)'true_A)'\nend\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(); saveat = tsteps))\n\n# Define the Neural Network\nnn = Chain(x -> x .^ 3, Dense(2, 16, tanh), Dense(16, 2))\np_init, st = Lux.setup(rng, nn)\n\nneuralode = NeuralODE(nn, tspan, Tsit5(); saveat = tsteps)\nprob_node = ODEProblem((u, p, t) -> nn(u, p, st)[1], u0, tspan, ComponentArray(p_init))\n\nfunction plot_multiple_shoot(plt, preds, group_size)\n    step = group_size - 1\n    ranges = group_ranges(datasize, group_size)\n\n    for (i, rg) in enumerate(ranges)\n        plot!(plt, tsteps[rg], preds[i][1, :]; markershape = :circle, label = \"Group $(i)\")\n    end\nend\n\nanim = Plots.Animation()\niter = 0\ncallback = function (p, l, preds; doplot = true)\n    display(l)\n    global iter\n    iter += 1\n    if doplot && iter % 1 == 0\n        # plot the original data\n        plt = scatter(tsteps, ode_data[1, :]; label = \"Data\")\n\n        # plot the different predictions for individual shoot\n        plot_multiple_shoot(plt, preds, group_size)\n\n        frame(anim)\n        display(plot(plt))\n    end\n    return false\nend\n\n# Define parameters for Multiple Shooting\ngroup_size = 3\ncontinuity_term = 200\n\nfunction loss_function(data, pred)\n    return sum(abs2, data - pred)\nend\n\nps = ComponentArray(p_init)\npd, pax = getdata(ps), getaxes(ps)\n\nfunction loss_multiple_shooting(p)\n    ps = ComponentArray(p, pax)\n    return multiple_shoot(ps, ode_data, tsteps, prob_node, loss_function, Tsit5(),\n        group_size; continuity_term)\nend\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x, p) -> loss_multiple_shooting(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, pd)\nres_ms = Optimization.solve(optprob, PolyOpt(); callback = callback)\ngif(anim, \"multiple_shooting.gif\"; fps = 15)","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"(Image: pic)","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"The connected lines show the predictions of each group. Notice that there are overlapping points as well. These are the points we are trying to coincide.","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"Here is an output with group_size = 30 (which is the same as solving on the whole interval without splitting also called single shooting).","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"(Image: pic_single_shoot3)","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"It is clear from the above picture, a single shoot doesn't perform very well with the ODE Problem we have and gets stuck in a local minimum.","category":"page"},{"location":"examples/neural_ode/#Neural-Ordinary-Differential-Equations","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"","category":"section"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"A neural ODE is an ODE where a neural network defines its derivative function. For example, with the multilayer perceptron neural network Lux.Chain(Lux.Dense(2, 50, tanh), Lux.Dense(50, 2)), we can define a differential equation which is u' = NN(u). This is done simply by the NeuralODE struct. Let's take a look at an example.","category":"page"},{"location":"examples/neural_ode/#Copy-Pasteable-Code","page":"Neural Ordinary Differential Equations","title":"Copy-Pasteable Code","text":"","category":"section"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"Before getting to the explanation, here's some code to start with. We will follow a full explanation of the definition and training process:","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"using ComponentArrays, Lux, DiffEqFlux, OrdinaryDiffEq, Optimization, OptimizationOptimJL,\n    OptimizationOptimisers, Random, Plots\n\nrng = Random.default_rng()\nu0 = Float32[2.0; 0.0]\ndatasize = 30\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2]; length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u .^ 3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(); saveat = tsteps))\n\ndudt2 = Chain(x -> x .^ 3, Dense(2, 50, tanh), Dense(50, 2))\np, st = Lux.setup(rng, dudt2)\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(); saveat = tsteps)\n\nfunction predict_neuralode(p)\n    Array(prob_neuralode(u0, p, st)[1])\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, ode_data .- pred)\n    return loss, pred\nend\n\n# Do not plot by default for the documentation\n# Users should change doplot=true to see the plots callbacks\ncallback = function (p, l, pred; doplot = false)\n    println(l)\n    # plot current prediction against data\n    if doplot\n        plt = scatter(tsteps, ode_data[1, :]; label = \"data\")\n        scatter!(plt, tsteps, pred[1, :]; label = \"prediction\")\n        display(plot(plt))\n    end\n    return false\nend\n\npinit = ComponentArray(p)\ncallback(pinit, loss_neuralode(pinit)...; doplot = true)\n\n# use Optimization.jl to solve the problem\nadtype = Optimization.AutoZygote()\n\noptf = Optimization.OptimizationFunction((x, p) -> loss_neuralode(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, pinit)\n\nresult_neuralode = Optimization.solve(optprob, Adam(0.05); callback = callback,\n    maxiters = 300)\n\noptprob2 = remake(optprob; u0 = result_neuralode.u)\n\nresult_neuralode2 = Optimization.solve(optprob2, Optim.BFGS(; initial_stepnorm = 0.01);\n    callback, allow_f_increases = false)\n\ncallback(result_neuralode2.u, loss_neuralode(result_neuralode2.u)...; doplot = true)","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"(Image: Neural ODE)","category":"page"},{"location":"examples/neural_ode/#Explanation","page":"Neural Ordinary Differential Equations","title":"Explanation","text":"","category":"section"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"Let's get a time series array from a spiral ODE to train against.","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"using ComponentArrays, Lux, DiffEqFlux, OrdinaryDiffEq, Optimization,\n    OptimizationOptimJL, OptimizationOptimisers, Random, Plots\n\nrng = Random.default_rng()\nu0 = Float32[2.0; 0.0]\ndatasize = 30\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2]; length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u .^ 3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(); saveat = tsteps))","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"Now let's define a neural network with a NeuralODE layer. First, we define the layer. Here we're going to use Lux.Chain, which is a suitable neural network structure for NeuralODEs with separate handling of state variables:","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"dudt2 = Chain(x -> x .^ 3, Dense(2, 50, tanh), Dense(50, 2))\np, st = Lux.setup(rng, dudt2)\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(); saveat = tsteps)","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"Note that we can directly use Chains from Lux.jl as well, for example:","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"dudt2 = Chain(x -> x .^ 3, Dense(2, 50, tanh), Dense(50, 2))","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"In our model, we used the x -> x.^3 assumption in the model. By incorporating structure into our equations, we can reduce the required size and training time for the neural network, but a good guess needs to be known!","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"From here we build a loss function around it. The NeuralODE has an optional second argument for new parameters, which we will use to change the neural network iteratively in our training loop. We will use the L2 loss of the network's output against the time series data:","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"function predict_neuralode(p)\n    Array(prob_neuralode(u0, p, st)[1])\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, ode_data .- pred)\n    return loss, pred\nend","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"We define a callback function. In this example, we set doplot = false because otherwise it would show every step and overflow the documentation, but for your use case set doplot=true to see a live animation of the training process!","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"# Callback function to observe training\ncallback = function (p, l, pred; doplot = false)\n    println(l)\n    # plot current prediction against data\n    if doplot\n        plt = scatter(tsteps, ode_data[1, :]; label = \"data\")\n        scatter!(plt, tsteps, pred[1, :]; label = \"prediction\")\n        display(plot(plt))\n    end\n    return false\nend\n\npinit = ComponentArray(p)\ncallback(pinit, loss_neuralode(pinit)...)","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"We then train the neural network to learn the ODE.","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"Here we showcase starting the optimization with Adam to more quickly find a minimum, and then honing in on the minimum by using LBFGS. By using the two together, we can fit the neural ODE in 9 seconds! (Note, the timing commented out the plotting). You can easily incorporate the procedure below to set up custom optimization problems. For more information on the usage of Optimization.jl, please consult this documentation.","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"The x and p variables in the optimization function are different from x and p above. The optimization function runs over the space of parameters of the original problem, so x_optimization == p_original.","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"# Train using the Adam optimizer\nadtype = Optimization.AutoZygote()\n\noptf = Optimization.OptimizationFunction((x, p) -> loss_neuralode(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, pinit)\n\nresult_neuralode = Optimization.solve(optprob,\n    Adam(0.05);\n    callback = callback,\n    maxiters = 300)","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"We then complete the training using a different optimizer, starting from where Adam stopped. We do allow_f_increases=false to make the optimization automatically halt when near the minimum.","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"# Retrain using the LBFGS optimizer\noptprob2 = remake(optprob; u0 = result_neuralode.u)\n\nresult_neuralode2 = Optimization.solve(optprob2,\n    Optim.BFGS(; initial_stepnorm = 0.01);\n    callback = callback,\n    allow_f_increases = false)","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"And then we use the callback with doplot=true to see the final plot:","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"callback(result_neuralode2.u, loss_neuralode(result_neuralode2.u)...; doplot = true)","category":"page"},{"location":"examples/GPUs/#Neural-ODEs-on-GPUs","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"","category":"section"},{"location":"examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"Note that the differential equation solvers will run on the GPU if the initial condition is a GPU array. Thus, for example, we can define a neural ODE manually that runs on the GPU (if no GPU is available, the calculation defaults back to the CPU).","category":"page"},{"location":"examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"For a detailed discussion on how GPUs need to be setup refer to Lux Docs.","category":"page"},{"location":"examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"using OrdinaryDiffEq, Lux, LuxCUDA, SciMLSensitivity, ComponentArrays\nusing Random\nrng = Random.default_rng()\n\nconst cdev = cpu_device()\nconst gdev = gpu_device()\n\nmodel = Chain(Dense(2, 50, tanh), Dense(50, 2))\nps, st = Lux.setup(rng, model)\nps = ps |> ComponentArray |> gdev\nst = st |> gdev\ndudt(u, p, t) = model(u, p, st)[1]\n\n# Simulation interval and intermediary points\ntspan = (0.0f0, 10.0f0)\ntsteps = 0.0f0:1.0f-1:10.0f0\n\nu0 = Float32[2.0; 0.0] |> gdev\nprob_gpu = ODEProblem(dudt, u0, tspan, ps)\n\n# Runs on a GPU\nsol_gpu = solve(prob_gpu, Tsit5(); saveat = tsteps)","category":"page"},{"location":"examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"Or we could directly use the neural ODE layer function, like:","category":"page"},{"location":"examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"using DiffEqFlux: NeuralODE\nprob_neuralode_gpu = NeuralODE(model, tspan, Tsit5(); saveat = tsteps)","category":"page"},{"location":"examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"If one is using Lux.Chain, then the computation takes place on the GPU with f(x,p,st) if x, p and st are on the GPU. This commonly looks like:","category":"page"},{"location":"examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"import Lux\n\ndudt2 = Chain(x -> x .^ 3, Dense(2, 50, tanh), Dense(50, 2))\n\nu0 = Float32[2.0; 0.0] |> gdev\np, st = Lux.setup(rng, dudt2) |> gdev\n\ndudt2_(u, p, t) = dudt2(u, p, st)[1]\n\n# Simulation interval and intermediary points\ntspan = (0.0f0, 10.0f0)\ntsteps = 0.0f0:1.0f-1:10.0f0\n\nprob_gpu = ODEProblem(dudt2_, u0, tspan, p)\n\n# Runs on a GPU\nsol_gpu = solve(prob_gpu, Tsit5(); saveat = tsteps)","category":"page"},{"location":"examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"or via the NeuralODE struct:","category":"page"},{"location":"examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"prob_neuralode_gpu = NeuralODE(dudt2, tspan, Tsit5(); saveat = tsteps)\nprob_neuralode_gpu(u0, p, st)","category":"page"},{"location":"examples/GPUs/#Neural-ODE-Example","page":"Neural ODEs on GPUs","title":"Neural ODE Example","text":"","category":"section"},{"location":"examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"Here is the full neural ODE example. Note that we use the gpu_device function so that the same code works on CPUs and GPUs, dependent on using LuxCUDA.","category":"page"},{"location":"examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"using Lux, Optimization, OptimizationOptimisers, Zygote, OrdinaryDiffEq,\n    Plots, LuxCUDA, SciMLSensitivity, Random, ComponentArrays\nimport DiffEqFlux: NeuralODE\n\nCUDA.allowscalar(false) # Makes sure no slow operations are occurring\n\n#rng for Lux.setup\nrng = Random.default_rng()\n# Generate Data\nu0 = Float32[2.0; 0.0]\ndatasize = 30\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2]; length = datasize)\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u .^ 3)'true_A)'\nend\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\n# Make the data into a GPU-based array if the user has a GPU\node_data = gdev(solve(prob_trueode, Tsit5(); saveat = tsteps))\n\ndudt2 = Chain(x -> x .^ 3, Dense(2, 50, tanh), Dense(50, 2))\nu0 = Float32[2.0; 0.0] |> gdev\np, st = Lux.setup(rng, dudt2)\np = p |> ComponentArray |> gdev\nst = st |> gdev\n\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(); saveat = tsteps)\n\nfunction predict_neuralode(p)\n    gdev(first(prob_neuralode(u0, p, st)))\nend\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, ode_data .- pred)\n    return loss, pred\nend\n# Callback function to observe training\nlist_plots = []\niter = 0\ncallback = function (p, l, pred; doplot = false)\n    global list_plots, iter\n    if iter == 0\n        list_plots = []\n    end\n    iter += 1\n    display(l)\n    # plot current prediction against data\n    plt = scatter(tsteps, Array(ode_data[1, :]); label = \"data\")\n    scatter!(plt, tsteps, Array(pred[1, :]); label = \"prediction\")\n    push!(list_plots, plt)\n    if doplot\n        display(plot(plt))\n    end\n    return false\nend\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x, p) -> loss_neuralode(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, p)\nresult_neuralode = Optimization.solve(optprob, Adam(0.05); callback, maxiters = 300)","category":"page"},{"location":"examples/normalizing_flows/#Continuous-Normalizing-Flows","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"Now, we study a single layer neural network that can estimate the density p_x of a variable of interest x by re-parameterizing a base variable z with known density p_z through the Neural Network model passed to the layer.","category":"page"},{"location":"examples/normalizing_flows/#Copy-Pasteable-Code","page":"Continuous Normalizing Flows","title":"Copy-Pasteable Code","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"Before getting to the explanation, here's some code to start with. We will follow a full explanation of the definition and training process:","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"using ComponentArrays, DiffEqFlux, OrdinaryDiffEq, Optimization, Distributions,\n    Random, OptimizationOptimisers, OptimizationOptimJL\n\nnn = Chain(Dense(1, 3, tanh), Dense(3, 1, tanh))\ntspan = (0.0f0, 10.0f0)\n\nffjord_mdl = FFJORD(nn, tspan, (1,), Tsit5(); ad = AutoZygote())\nps, st = Lux.setup(Random.default_rng(), ffjord_mdl)\nps = ComponentArray(ps)\nmodel = Lux.Experimental.StatefulLuxLayer(ffjord_mdl, nothing, st)\n\n# Training\ndata_dist = Normal(6.0f0, 0.7f0)\ntrain_data = Float32.(rand(data_dist, 1, 100))\n\nfunction loss(θ)\n    logpx, λ₁, λ₂ = model(train_data, θ)\n    return -mean(logpx)\nend\n\nfunction cb(p, l)\n    @info \"FFJORD Training\" loss=loss(p)\n    return false\nend\n\nadtype = Optimization.AutoForwardDiff()\noptf = Optimization.OptimizationFunction((x, p) -> loss(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, ps)\n\nres1 = Optimization.solve(optprob, Adam(0.01); maxiters = 20, callback = cb)\n\noptprob2 = Optimization.OptimizationProblem(optf, res1.u)\nres2 = Optimization.solve(optprob2, Optim.LBFGS(); allow_f_increases = false,\n    callback = cb)\n\n# Evaluation\nusing Distances\n\nst_ = (; st..., monte_carlo = false)\n\nactual_pdf = pdf.(data_dist, train_data)\nlearned_pdf = exp.(ffjord_mdl(train_data, res2.u, st_)[1][1])\ntrain_dis = totalvariation(learned_pdf, actual_pdf) / size(train_data, 2)\n\n# Data Generation\nffjord_dist = FFJORDDistribution(ffjord_mdl, ps, st)\nnew_data = rand(ffjord_dist, 100)","category":"page"},{"location":"examples/normalizing_flows/#Step-by-Step-Explanation","page":"Continuous Normalizing Flows","title":"Step-by-Step Explanation","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"We can use DiffEqFlux.jl to define, train and output the densities computed by CNF layers. In the same way as a neural ODE, the layer takes a neural network that defines its derivative function (see [1] for a reference). A possible way to define a CNF layer, would be:","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"using ComponentArrays, DiffEqFlux, OrdinaryDiffEq, Optimization, OptimizationOptimisers,\n    OptimizationOptimJL, Distributions, Random\n\nnn = Chain(Dense(1, 3, tanh), Dense(3, 1, tanh))\ntspan = (0.0f0, 10.0f0)\n\nffjord_mdl = FFJORD(nn, tspan, (1,), Tsit5(); ad = AutoZygote())\nps, st = Lux.setup(Random.default_rng(), ffjord_mdl)\nps = ComponentArray(ps)\nmodel = Lux.Experimental.StatefulLuxLayer(ffjord_mdl, ps, st)\nffjord_mdl","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"where we also pass as an input the desired timespan for which the differential equation that defines log p_x and z(t) will be solved.","category":"page"},{"location":"examples/normalizing_flows/#Training","page":"Continuous Normalizing Flows","title":"Training","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"First, let's get an array from a normal distribution as the training data. Note that we want the data in Float32 values to match how we have set up the neural network weights and the state space of the ODE.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"data_dist = Normal(6.0f0, 0.7f0)\ntrain_data = Float32.(rand(data_dist, 1, 100))","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"Now we define a loss function that we wish to minimize and a callback function to track loss improvements","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"function loss(θ)\n    logpx, λ₁, λ₂ = model(train_data, θ)\n    return -mean(logpx)\nend\n\nfunction cb(p, l)\n    @info \"FFJORD Training\" loss=loss(p)\n    return false\nend","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"In this example, we wish to choose the parameters of the network such that the likelihood of the re-parameterized variable is maximized. Other loss functions may be used depending on the application. Furthermore, the CNF layer gives the log of the density of the variable x, as one may guess from the code above.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"We then train the neural network to learn the distribution of x.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"Here we showcase starting the optimization with Adam to more quickly find a minimum, and then honing in on the minimum by using LBFGS.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"\nadtype = Optimization.AutoForwardDiff()\noptf = Optimization.OptimizationFunction((x, p) -> loss(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, ps)\n\nres1 = Optimization.solve(optprob, Adam(0.01); maxiters = 20, callback = cb)","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"We then complete the training using a different optimizer, starting from where Adam stopped.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"optprob2 = Optimization.OptimizationProblem(optf, res1.u)\nres2 = Optimization.solve(optprob2, Optim.LBFGS(); allow_f_increases = false,\n    callback = cb)","category":"page"},{"location":"examples/normalizing_flows/#Evaluation","page":"Continuous Normalizing Flows","title":"Evaluation","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"For evaluating the result, we can use totalvariation function from Distances.jl. First, we compute densities using actual distribution and FFJORD model. Then we use a distance function between these distributions.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"using Distances\n\nst_ = (; st..., monte_carlo = false)\n\nactual_pdf = pdf.(data_dist, train_data)\nlearned_pdf = exp.(ffjord_mdl(train_data, res2.u, st_)[1][1])\ntrain_dis = totalvariation(learned_pdf, actual_pdf) / size(train_data, 2)","category":"page"},{"location":"examples/normalizing_flows/#Data-Generation","page":"Continuous Normalizing Flows","title":"Data Generation","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"What's more, we can generate new data by using FFJORD as a distribution in rand.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"ffjord_dist = FFJORDDistribution(ffjord_mdl, ps, st)\nnew_data = rand(ffjord_dist, 100)","category":"page"},{"location":"examples/normalizing_flows/#References","page":"Continuous Normalizing Flows","title":"References","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"[1] Grathwohl, Will, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. \"Ffjord: Free-form continuous dynamics for scalable reversible generative models.\" arXiv preprint arXiv:1810.01367 (2018).","category":"page"},{"location":"examples/mnist_neural_ode/#mnist","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Training a classifier for MNIST using a neural ordinary differential equation NN-ODE on GPUs with minibatching.","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"(Step-by-step description below)","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"using DiffEqFlux, CUDA, Zygote, MLDataUtils, NNlib, OrdinaryDiffEq, Test, Lux, Statistics,\n    ComponentArrays, Random, Optimization, OptimizationOptimisers, LuxCUDA\nusing MLDatasets: MNIST\nusing MLDataUtils: LabelEnc, convertlabel, stratifiedobs\n\nCUDA.allowscalar(false)\nENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true\n\nconst cdev = cpu_device()\nconst gdev = gpu_device()\n\nlogitcrossentropy(ŷ, y) = mean(-sum(y .* logsoftmax(ŷ; dims = 1); dims = 1))\n\nfunction loadmnist(batchsize = bs)\n    # Use MLDataUtils LabelEnc for natural onehot conversion\n    function onehot(labels_raw)\n        convertlabel(LabelEnc.OneOfK, labels_raw, LabelEnc.NativeLabels(collect(0:9)))\n    end\n    # Load MNIST\n    mnist = MNIST(; split = :train)\n    imgs, labels_raw = mnist.features, mnist.targets\n    # Process images into (H,W,C,BS) batches\n    x_train = Float32.(reshape(imgs, size(imgs, 1), size(imgs, 2), 1, size(imgs, 3))) |>\n              gdev\n    x_train = batchview(x_train, batchsize)\n    # Onehot and batch the labels\n    y_train = onehot(labels_raw) |> gdev\n    y_train = batchview(y_train, batchsize)\n    return x_train, y_train\nend\n\n# Main\nconst bs = 128\nx_train, y_train = loadmnist(bs)\n\ndown = Lux.Chain(Lux.FlattenLayer(), Lux.Dense(784, 20, tanh))\nnn = Lux.Chain(Lux.Dense(20, 10, tanh), Lux.Dense(10, 10, tanh),\n    Lux.Dense(10, 20, tanh))\nfc = Lux.Dense(20, 10)\n\nnn_ode = NeuralODE(nn, (0.0f0, 1.0f0), Tsit5(); save_everystep = false, reltol = 1e-3,\n    abstol = 1e-3, save_start = false)\n\nfunction DiffEqArray_to_Array(x)\n    xarr = gdev(x)\n    return reshape(xarr, size(xarr)[1:2])\nend\n\n#Build our over-all model topology\nm = Lux.Chain(; down, nn_ode, convert = Lux.WrappedFunction(DiffEqArray_to_Array), fc)\nps, st = Lux.setup(Random.default_rng(), m)\nps = ComponentArray(ps) |> gdev\nst = st |> gdev\n\n#We can also build the model topology without a NN-ODE\nm_no_ode = Lux.Chain(; down, nn, fc)\nps_no_ode, st_no_ode = Lux.setup(Random.default_rng(), m_no_ode)\nps_no_ode = ComponentArray(ps_no_ode) |> gdev\nst_no_ode = st_no_ode |> gdev\n\n#To understand the intermediate NN-ODE layer, we can examine it's dimensionality\nx_d = first(down(x_train[1], ps.down, st.down))\n\n# We can see that we can compute the forward pass through the NN topology featuring an NNODE layer.\nx_m = first(m(x_train[1], ps, st))\n#Or without the NN-ODE layer.\nx_m = first(m_no_ode(x_train[1], ps_no_ode, st_no_ode))\n\nclassify(x) = argmax.(eachcol(x))\n\nfunction accuracy(model, data, ps, st; n_batches = 100)\n    total_correct = 0\n    total = 0\n    st = Lux.testmode(st)\n    for (x, y) in collect(data)[1:n_batches]\n        target_class = classify(cdev(y))\n        predicted_class = classify(cdev(first(model(x, ps, st))))\n        total_correct += sum(target_class .== predicted_class)\n        total += length(target_class)\n    end\n    return total_correct / total\nend\n#burn in accuracy\naccuracy(m, zip(x_train, y_train), ps, st)\n\nfunction loss_function(ps, x, y)\n    pred, st_ = m(x, ps, st)\n    return logitcrossentropy(pred, y), pred\nend\n\n#burn in loss\nloss_function(ps, x_train[1], y_train[1])\n\nopt = OptimizationOptimisers.Adam(0.05)\niter = 0\n\nopt_func = OptimizationFunction((ps, _, x, y) -> loss_function(ps, x, y),\n    Optimization.AutoZygote())\nopt_prob = OptimizationProblem(opt_func, ps)\n\nfunction callback(ps, l, pred)\n    global iter += 1\n    #Monitor that the weights do infact update\n    #Every 10 training iterations show accuracy\n    if (iter % 10 == 0)\n        @info \"[MNIST GPU] Accuracy: $(accuracy(m, zip(x_train, y_train), ps, st))\"\n    end\n    return false\nend\n\n# Train the NN-ODE and monitor the loss and weights.\nres = Optimization.solve(opt_prob, opt, zip(x_train, y_train); callback)\n@test accuracy(m, zip(x_train, y_train), res.u, st) > 0.8","category":"page"},{"location":"examples/mnist_neural_ode/#Step-by-Step-Description","page":"GPU-based MNIST Neural ODE Classifier","title":"Step-by-Step Description","text":"","category":"section"},{"location":"examples/mnist_neural_ode/#Load-Packages","page":"GPU-based MNIST Neural ODE Classifier","title":"Load Packages","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"using DiffEqFlux, CUDA, Zygote, MLDataUtils, NNlib, OrdinaryDiffEq, Test, Lux, Statistics,\n    ComponentArrays, Random, Optimization, OptimizationOptimisers, LuxCUDA\nusing MLDatasets: MNIST\nusing MLDataUtils: LabelEnc, convertlabel, stratifiedobs","category":"page"},{"location":"examples/mnist_neural_ode/#GPU","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"A good trick used here:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"\nCUDA.allowscalar(false)\nENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true\n\nconst cdev = cpu_device()\nconst gdev = gpu_device()","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"ensures that only optimized kernels are called when using the GPU. Additionally, the gpu_device function is shown as a way to translate models and data over to the GPU. Note that this function is CPU-safe, so if the GPU is disabled or unavailable, this code will fall back to the CPU.","category":"page"},{"location":"examples/mnist_neural_ode/#Load-MNIST-Dataset-into-Minibatches","page":"GPU-based MNIST Neural ODE Classifier","title":"Load MNIST Dataset into Minibatches","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"The MNIST dataset is split into 60.000 train and 10.000 test images, ensuring a balanced ratio of labels.","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"The preprocessing is done in loadmnist where the raw MNIST data is split into features x and labels y. Features are reshaped into format [Height, Width, Color, Samples], in case of the train set [28, 28, 1, 60000]. Using Flux's onehotbatch function, the labels (numbers 0 to 9) are one-hot encoded, resulting in a a [10, 60000] OneHotMatrix.","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Features and labels are then passed to Flux's DataLoader. This automatically minibatches both the images and labels using the specified batchsize, meaning that every minibatch will contain 128 images with a single color channel of 28x28 pixels. Additionally, it allows us to shuffle the train dataset in each epoch.","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"logitcrossentropy(ŷ, y) = mean(-sum(y .* logsoftmax(ŷ; dims = 1); dims = 1))\n\nfunction loadmnist(batchsize = bs)\n    # Use MLDataUtils LabelEnc for natural onehot conversion\n    function onehot(labels_raw)\n        convertlabel(LabelEnc.OneOfK, labels_raw, LabelEnc.NativeLabels(collect(0:9)))\n    end\n    # Load MNIST\n    mnist = MNIST(; split = :train)\n    imgs, labels_raw = mnist.features, mnist.targets\n    # Process images into (H,W,C,BS) batches\n    x_train = Float32.(reshape(imgs, size(imgs, 1), size(imgs, 2), 1, size(imgs, 3))) |>\n              gdev\n    x_train = batchview(x_train, batchsize)\n    # Onehot and batch the labels\n    y_train = onehot(labels_raw) |> gdev\n    y_train = batchview(y_train, batchsize)\n    return x_train, y_train\nend","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"and then loaded from main:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"# Main\nconst bs = 128\nx_train, y_train = loadmnist(bs)","category":"page"},{"location":"examples/mnist_neural_ode/#Layers","page":"GPU-based MNIST Neural ODE Classifier","title":"Layers","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"The Neural Network requires passing inputs sequentially through multiple layers. We use Chain which allows inputs to functions to come from the previous layer and sends the outputs to the next. Four different sets of layers are used here:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"\ndown = Lux.Chain(Lux.FlattenLayer(), Lux.Dense(784, 20, tanh))\nnn = Lux.Chain(Lux.Dense(20, 10, tanh), Lux.Dense(10, 10, tanh),\n    Lux.Dense(10, 20, tanh))\nfc = Lux.Dense(20, 10)","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"down: This layer downsamples our images into a 20 dimensional feature vector. It takes a 28 x 28 image, flattens it, and then passes it through a fully connected layer with tanh activation","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"nn: A 3 layers Deep Neural Network Chain with tanh activation which is used to model our differential equation","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"nn_ode: ODE solver layer","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"fc: The final fully connected layer which maps our learned feature vector to the probability of the feature vector of belonging to a particular class","category":"page"},{"location":"examples/mnist_neural_ode/#Array-Conversion","page":"GPU-based MNIST Neural ODE Classifier","title":"Array Conversion","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"When using NeuralODE, this function converts the ODESolution's DiffEqArray to a Matrix (CuArray), and reduces the matrix from 3 to 2 dimensions for use in the next layer.","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"nn_ode = NeuralODE(nn, (0.0f0, 1.0f0), Tsit5(); save_everystep = false, reltol = 1e-3,\n    abstol = 1e-3, save_start = false)\n\nfunction DiffEqArray_to_Array(x)\n    xarr = gdev(x)\n    return reshape(xarr, size(xarr)[1:2])\nend","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"For CPU: If this function does not automatically fall back to CPU when no GPU is present, we can change gdev(x) to Array(x).","category":"page"},{"location":"examples/mnist_neural_ode/#Build-Topology","page":"GPU-based MNIST Neural ODE Classifier","title":"Build Topology","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Next, we connect all layers together in a single chain:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"# Build our overall model topology\nm = Lux.Chain(; down, nn_ode, convert = Lux.WrappedFunction(DiffEqArray_to_Array), fc)\nps, st = Lux.setup(Random.default_rng(), m)\nps = ComponentArray(ps) |> gdev\nst = st |> gdev","category":"page"},{"location":"examples/mnist_neural_ode/#Prediction","page":"GPU-based MNIST Neural ODE Classifier","title":"Prediction","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"To convert the classification back into readable numbers, we use classify which returns the prediction by taking the arg max of the output for each column of the minibatch:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"classify(x) = argmax.(eachcol(x))","category":"page"},{"location":"examples/mnist_neural_ode/#Accuracy","page":"GPU-based MNIST Neural ODE Classifier","title":"Accuracy","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"We then evaluate the accuracy on n_batches at a time through the entire network:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"function accuracy(model, data, ps, st; n_batches = 100)\n    total_correct = 0\n    total = 0\n    st = Lux.testmode(st)\n    for (x, y) in collect(data)[1:n_batches]\n        target_class = classify(cdev(y))\n        predicted_class = classify(cdev(first(model(x, ps, st))))\n        total_correct += sum(target_class .== predicted_class)\n        total += length(target_class)\n    end\n    return total_correct / total\nend\n#burn in accuracy\naccuracy(m, zip(x_train, y_train), ps, st)","category":"page"},{"location":"examples/mnist_neural_ode/#Training-Parameters","page":"GPU-based MNIST Neural ODE Classifier","title":"Training Parameters","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Once we have our model, we can train our neural network by backpropagation using Flux.train!. This function requires Loss, Optimizer and Callback functions.","category":"page"},{"location":"examples/mnist_neural_ode/#Loss","page":"GPU-based MNIST Neural ODE Classifier","title":"Loss","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Cross Entropy is the loss function computed here, which applies a Softmax operation on the final output of our model. logitcrossentropy takes in the prediction from our model model(x) and compares it to actual output y:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"function loss_function(ps, x, y)\n    pred, st_ = m(x, ps, st)\n    return logitcrossentropy(pred, y), pred\nend\n\n#burn in loss\nloss_function(ps, x_train[1], y_train[1])","category":"page"},{"location":"examples/mnist_neural_ode/#Optimizer","page":"GPU-based MNIST Neural ODE Classifier","title":"Optimizer","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Adam is specified here as our optimizer with a learning rate of 0.05:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"opt = OptimizationOptimisers.Adam(0.05)","category":"page"},{"location":"examples/mnist_neural_ode/#CallBack","page":"GPU-based MNIST Neural ODE Classifier","title":"CallBack","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"This callback function is used to print both the training and testing accuracy after 10 training iterations:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"iter = 0\n\nopt_func = OptimizationFunction((ps, _, x, y) -> loss_function(ps, x, y),\n    Optimization.AutoZygote())\nopt_prob = OptimizationProblem(opt_func, ps)\n\nfunction callback(ps, l, pred)\n    global iter += 1\n    #Monitor that the weights do infact update\n    #Every 10 training iterations show accuracy\n    if (iter % 10 == 0)\n        @info \"[MNIST GPU] Accuracy: $(accuracy(m, zip(x_train, y_train), ps, st))\"\n    end\n    return false\nend","category":"page"},{"location":"examples/mnist_neural_ode/#Train","page":"GPU-based MNIST Neural ODE Classifier","title":"Train","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"To train our model, we select the appropriate trainable parameters of our network with params. In our case, backpropagation is required for down, nn_ode and fc. Notice that the parameters for Neural ODE is given by nn_ode.p:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"# Train the NN-ODE and monitor the loss and weights.\nres = Optimization.solve(opt_prob, opt, zip(x_train, y_train); callback)\n@test accuracy(m, zip(x_train, y_train), res.u, st) > 0.8","category":"page"},{"location":"examples/mnist_neural_ode/#Expected-Output","page":"GPU-based MNIST Neural ODE Classifier","title":"Expected Output","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"[ Info: [MNIST GPU] Accuracy: 0.602734375\n[ Info: [MNIST GPU] Accuracy: 0.719609375\n[ Info: [MNIST GPU] Accuracy: 0.783671875\n[ Info: [MNIST GPU] Accuracy: 0.8171875\n[ Info: [MNIST GPU] Accuracy: 0.82390625\n[ Info: [MNIST GPU] Accuracy: 0.840546875\n[ Info: [MNIST GPU] Accuracy: 0.839765625\n[ Info: [MNIST GPU] Accuracy: 0.843046875\n[ Info: [MNIST GPU] Accuracy: 0.8609375\n[ Info: [MNIST GPU] Accuracy: 0.86\n[ Info: [MNIST GPU] Accuracy: 0.866875\n[ Info: [MNIST GPU] Accuracy: 0.86484375\n[ Info: [MNIST GPU] Accuracy: 0.883515625\n[ Info: [MNIST GPU] Accuracy: 0.87046875\n[ Info: [MNIST GPU] Accuracy: 0.87609375\n[ Info: [MNIST GPU] Accuracy: 0.880703125\n[ Info: [MNIST GPU] Accuracy: 0.874609375\n[ Info: [MNIST GPU] Accuracy: 0.870859375\n[ Info: [MNIST GPU] Accuracy: 0.881640625\n[ Info: [MNIST GPU] Accuracy: 0.887734375\n[ Info: [MNIST GPU] Accuracy: 0.88734375\n[ Info: [MNIST GPU] Accuracy: 0.880078125\n[ Info: [MNIST GPU] Accuracy: 0.88078125\n[ Info: [MNIST GPU] Accuracy: 0.88125\n[ Info: [MNIST GPU] Accuracy: 0.87203125\n[ Info: [MNIST GPU] Accuracy: 0.857890625\n[ Info: [MNIST GPU] Accuracy: 0.87203125\n[ Info: [MNIST GPU] Accuracy: 0.877578125\n[ Info: [MNIST GPU] Accuracy: 0.879765625\n[ Info: [MNIST GPU] Accuracy: 0.885703125\n[ Info: [MNIST GPU] Accuracy: 0.895\n[ Info: [MNIST GPU] Accuracy: 0.90171875\n[ Info: [MNIST GPU] Accuracy: 0.893359375\n[ Info: [MNIST GPU] Accuracy: 0.882109375\n[ Info: [MNIST GPU] Accuracy: 0.87453125\n[ Info: [MNIST GPU] Accuracy: 0.881171875\n[ Info: [MNIST GPU] Accuracy: 0.891171875\n[ Info: [MNIST GPU] Accuracy: 0.899921875\n[ Info: [MNIST GPU] Accuracy: 0.89890625\n[ Info: [MNIST GPU] Accuracy: 0.895078125\n[ Info: [MNIST GPU] Accuracy: 0.89171875\n[ Info: [MNIST GPU] Accuracy: 0.899296875\n[ Info: [MNIST GPU] Accuracy: 0.891484375\n[ Info: [MNIST GPU] Accuracy: 0.899375\n[ Info: [MNIST GPU] Accuracy: 0.88953125\n[ Info: [MNIST GPU] Accuracy: 0.88890625","category":"page"},{"location":"examples/neural_sde/#Neural-Stochastic-Differential-Equations-With-Method-of-Moments","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"","category":"section"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"With neural stochastic differential equations, there is once again a helper form neural_dmsde which can be used for the multiplicative noise case (consult the layers API documentation, or this full example using the layer function).","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"However, since there are far too many possible combinations for the API to support, often you will want to define neural differential equations for non-ODE systems from scratch. To get good performance for these systems, it is generally best to use TrackerAdjoint with non-mutating (out-of-place) forms. For example, the following defines a neural SDE with neural networks for both the drift and diffusion terms:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"dudt(u, p, t) = model(u)\ng(u, p, t) = model2(u)\nprob = SDEProblem(dudt, g, x, tspan, nothing)","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"where model and model2 are different neural networks. The same can apply to a neural delay differential equation. Its out-of-place formulation is f(u,h,p,t). Thus, for example, if we want to define a neural delay differential equation which uses the history value at p.tau in the past, we can define:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"dudt!(u, h, p, t) = model([u; h(t - p.tau)])\nprob = DDEProblem(dudt_, u0, h, tspan, nothing)","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"First, let's build training data from the same example as the neural ODE:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"using Plots, Statistics\nusing ComponentArrays, Optimization,\n    OptimizationOptimisers, DiffEqFlux, StochasticDiffEq, SciMLBase.EnsembleAnalysis, Random\n\nu0 = Float32[2.0; 0.0]\ndatasize = 30\ntspan = (0.0f0, 1.0f0)\ntsteps = range(tspan[1], tspan[2]; length = datasize)","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"function trueSDEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u .^ 3)'true_A)'\nend\n\nmp = Float32[0.2, 0.2]\nfunction true_noise_func(du, u, p, t)\n    du .= mp .* u\nend\n\nprob_truesde = SDEProblem(trueSDEfunc, true_noise_func, u0, tspan)","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"For our dataset, we will use DifferentialEquations.jl's parallel ensemble interface to generate data from the average of 10,000 runs of the SDE:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"# Take a typical sample from the mean\nensemble_prob = EnsembleProblem(prob_truesde; safetycopy = false)\nensemble_sol = solve(ensemble_prob, SOSRI(); trajectories = 10000)\nensemble_sum = EnsembleSummary(ensemble_sol)\n\nsde_data, sde_data_vars = Array.(timeseries_point_meanvar(ensemble_sol, tsteps))","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"Now we build a neural SDE. For simplicity, we will use the NeuralDSDE neural SDE with diagonal noise layer function:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"drift_dudt = Chain(x -> x .^ 3, Dense(2, 50, tanh), Dense(50, 2))\ndiffusion_dudt = Dense(2, 2)\n\nneuralsde = NeuralDSDE(drift_dudt, diffusion_dudt, tspan, SOSRI();\n    saveat = tsteps, reltol = 1e-1, abstol = 1e-1)\nps, st = Lux.setup(Random.default_rng(), neuralsde)\nps = ComponentArray(ps)","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"Let's see what that looks like:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"# Get the prediction using the correct initial condition\nprediction0 = neuralsde(u0, ps, st)[1]\n\ndrift_model = Lux.Experimental.StatefulLuxLayer(drift_dudt, nothing, st.drift)\ndiffusion_model = Lux.Experimental.StatefulLuxLayer(diffusion_dudt, nothing, st.diffusion)\n\ndrift_(u, p, t) = drift_model(u, p.drift)\ndiffusion_(u, p, t) = diffusion_model(u, p.diffusion)\n\nprob_neuralsde = SDEProblem(drift_, diffusion_, u0, (0.0f0, 1.2f0), ps)\n\nensemble_nprob = EnsembleProblem(prob_neuralsde; safetycopy = false)\nensemble_nsol = solve(ensemble_nprob, SOSRI(); trajectories = 100, saveat = tsteps)\nensemble_nsum = EnsembleSummary(ensemble_nsol)\n\nplt1 = plot(ensemble_nsum; title = \"Neural SDE: Before Training\")\nscatter!(plt1, tsteps, sde_data'; lw = 3)\n\nscatter(tsteps, sde_data[1, :]; label = \"data\")\nscatter!(tsteps, prediction0[1, :]; label = \"prediction\")","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"Now just as with the neural ODE we define a loss function that calculates the mean and variance from n runs at each time point and uses the distance from the data values:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"neuralsde_model = Lux.Experimental.StatefulLuxLayer(neuralsde, nothing, st)\n\nfunction predict_neuralsde(p, u = u0)\n    return Array(neuralsde_model(u, p))\nend\n\nfunction loss_neuralsde(p; n = 100)\n    u = repeat(reshape(u0, :, 1), 1, n)\n    samples = predict_neuralsde(p, u)\n    means = mean(samples; dims = 2)\n    vars = var(samples; dims = 2, mean = means)[:, 1, :]\n    means = means[:, 1, :]\n    loss = sum(abs2, sde_data - means) + sum(abs2, sde_data_vars - vars)\n    return loss, means, vars\nend","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"list_plots = []\niter = 0\n\n# Callback function to observe training\ncallback = function (p, loss, means, vars; doplot = false)\n    global list_plots, iter\n\n    if iter == 0\n        list_plots = []\n    end\n    iter += 1\n\n    # loss against current data\n    display(loss)\n\n    # plot current prediction against data\n    plt = Plots.scatter(tsteps, sde_data[1, :]; yerror = sde_data_vars[1, :],\n        ylim = (-4.0, 8.0), label = \"data\")\n    Plots.scatter!(plt, tsteps, means[1, :]; ribbon = vars[1, :], label = \"prediction\")\n    push!(list_plots, plt)\n\n    if doplot\n        display(plt)\n    end\n    return false\nend","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"Now we train using this loss function. We can pre-train a little bit using a smaller n and then decrease it after it has had some time to adjust towards the right mean behavior:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"opt = Adam(0.025)\n\n# First round of training with n = 10\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x, p) -> loss_neuralsde(x; n = 10), adtype)\noptprob = Optimization.OptimizationProblem(optf, ps)\nresult1 = Optimization.solve(optprob, opt; callback, maxiters = 100)","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"We resume the training with a larger n. (WARNING - this step is a couple of orders of magnitude longer than the previous one).","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"optf2 = Optimization.OptimizationFunction((x, p) -> loss_neuralsde(x; n = 100), adtype)\noptprob2 = Optimization.OptimizationProblem(optf2, result1.u)\nresult2 = Optimization.solve(optprob2, opt; callback, maxiters = 20)","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"And now we plot the solution to an ensemble of the trained neural SDE:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"_, means, vars = loss_neuralsde(result2.u; n = 1000)\n\nplt2 = Plots.scatter(tsteps, sde_data'; yerror = sde_data_vars',\n    label = \"data\", title = \"Neural SDE: After Training\",\n    xlabel = \"Time\")\nplot!(plt2, tsteps, means'; lw = 8, ribbon = vars', label = \"prediction\")\n\nplt = plot(plt1, plt2; layout = (2, 1))\nsavefig(plt, \"NN_sde_combined.png\");\nnothing; # sde","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"(Image: Neural SDE Trained Example)","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"Try this with GPUs as well!","category":"page"},{"location":"examples/collocation/#Smoothed-Collocation-for-Fast-Two-Stage-Training","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"","category":"section"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"note: Note\nThis is one of many methods for calculating the collocation coefficients for the training process. For a more comprehensive set of collocation methods, see the JuliaSimModelOptimizer.","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"One can avoid a lot of the computational cost of the ODE solver by pretraining the neural network against a smoothed collocation of the data. First the example and then an explanation.","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"using ComponentArrays, Lux, DiffEqFlux, OrdinaryDiffEq, SciMLSensitivity, Optimization,\n    OptimizationOptimisers, Plots\n\nusing Random\nrng = Random.default_rng()\n\nu0 = Float32[2.0; 0.0]\ndatasize = 300\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2]; length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u .^ 3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\ndata = Array(solve(prob_trueode, Tsit5(); saveat = tsteps)) .+ 0.1randn(2, 300)\n\ndu, u = collocate_data(data, tsteps, EpanechnikovKernel())\n\nscatter(tsteps, data')\nplot!(tsteps, u'; lw = 5)\nsavefig(\"colloc.png\")\nplot(tsteps, du')\nsavefig(\"colloc_du.png\")\n\ndudt2 = Chain(x -> x .^ 3, Dense(2, 50, tanh), Dense(50, 2))\n\nfunction loss(p)\n    cost = zero(first(p))\n    for i in 1:size(du, 2)\n        _du, _ = dudt2(@view(u[:, i]), p, st)\n        dui = @view du[:, i]\n        cost += sum(abs2, dui .- _du)\n    end\n    sqrt(cost)\nend\n\npinit, st = Lux.setup(rng, dudt2)\n\ncallback = function (p, l)\n    return false\nend\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x, p) -> loss(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, ComponentArray(pinit))\n\nresult_neuralode = Optimization.solve(optprob, Adam(0.05); callback, maxiters = 10000)\n\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(); saveat = tsteps)\nnn_sol, st = prob_neuralode(u0, result_neuralode.u, st)\nscatter(tsteps, data')\nplot!(nn_sol)\nsavefig(\"colloc_trained.png\")\n\nfunction predict_neuralode(p)\n    Array(prob_neuralode(u0, p, st)[1])\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, data .- pred)\n    return loss\nend\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x, p) -> loss_neuralode(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, ComponentArray(pinit))\n\nnumerical_neuralode = Optimization.solve(optprob, Adam(0.05); callback, maxiters = 300)\n\nnn_sol, st = prob_neuralode(u0, numerical_neuralode.u, st)\nscatter(tsteps, data')\nplot!(nn_sol; lw = 5)","category":"page"},{"location":"examples/collocation/#Generating-the-Collocation","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Generating the Collocation","text":"","category":"section"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"The smoothed collocation is a spline fit of the data points which allows us to get an estimate of the approximate noiseless dynamics:","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"using ComponentArrays,\n    Lux, DiffEqFlux, Optimization, OptimizationOptimisers, OrdinaryDiffEq, Plots\n\nusing Random\nrng = Random.default_rng()\n\nu0 = Float32[2.0; 0.0]\ndatasize = 300\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2]; length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u .^ 3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\ndata = Array(solve(prob_trueode, Tsit5(); saveat = tsteps)) .+ 0.1randn(2, 300)\n\ndu, u = collocate_data(data, tsteps, EpanechnikovKernel())\n\nscatter(tsteps, data')\nplot!(tsteps, u'; lw = 5)","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"We can then differentiate the smoothed function to get estimates of the derivative at each data point:","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"plot(tsteps, du')","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"Because we have (u',u) pairs, we can write a loss function that calculates the squared difference between f(u,p,t) and u' at each point, and find the parameters which minimize this difference:","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"dudt2 = Chain(x -> x .^ 3, Dense(2, 50, tanh), Dense(50, 2))\n\nfunction loss(p)\n    cost = zero(first(p))\n    for i in 1:size(du, 2)\n        _du, _ = dudt2(@view(u[:, i]), p, st)\n        dui = @view du[:, i]\n        cost += sum(abs2, dui .- _du)\n    end\n    sqrt(cost)\nend\n\npinit, st = Lux.setup(rng, dudt2)\n\ncallback = function (p, l)\n    return false\nend\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x, p) -> loss(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, ComponentArray(pinit))\n\nresult_neuralode = Optimization.solve(optprob, Adam(0.05); callback, maxiters = 10000)\n\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(); saveat = tsteps)\nnn_sol, st = prob_neuralode(u0, result_neuralode.u, st)\nscatter(tsteps, data')\nplot!(nn_sol)","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"While this doesn't look great, it has the characteristics of the full solution all throughout the timeseries, but it does have a drift. We can continue to optimize like this, or we can use this as the initial condition to the next phase of our fitting:","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"function predict_neuralode(p)\n    Array(prob_neuralode(u0, p, st)[1])\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, data .- pred)\n    return loss\nend\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x, p) -> loss_neuralode(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, ComponentArray(pinit))\n\nnumerical_neuralode = Optimization.solve(optprob, Adam(0.05); callback, maxiters = 300)\n\nnn_sol, st = prob_neuralode(u0, numerical_neuralode.u, st)\nscatter(tsteps, data')\nplot!(nn_sol; lw = 5)","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"This method then has a good global starting position, making it less prone to local minima, and this method is thus a great method to mix in with other fitting methods for neural ODEs.","category":"page"},{"location":"examples/hamiltonian_nn/#Hamiltonian-Neural-Network","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"","category":"section"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"Hamiltonian Neural Networks introduced in [1] allow models to \"learn and respect exact conservation laws in an unsupervised manner\". In this example, we will train a model to learn the Hamiltonian for a 1D Spring mass system. This system is described by the equation:","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"mddot x + kx = 0","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"Now we make some simplifying assumptions, and assign m = 1 and k = 1. Analytically solving this equation, we get x = sin(t). Hence, q = sin(t), and p = cos(t). Using these solutions, we generate our dataset and fit the NeuralHamiltonianDE to learn the dynamics of this system.","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"using Lux, DiffEqFlux, OrdinaryDiffEq, Statistics, Plots, Zygote, ForwardDiff, Random,\n    ComponentArrays, Optimization, OptimizationOptimisers, IterTools\n\nt = range(0.0f0, 1.0f0; length = 1024)\nπ_32 = Float32(π)\nq_t = reshape(sin.(2π_32 * t), 1, :)\np_t = reshape(cos.(2π_32 * t), 1, :)\ndqdt = 2π_32 .* p_t\ndpdt = -2π_32 .* q_t\n\ndata = vcat(q_t, p_t)\ntarget = vcat(dqdt, dpdt)\nB = 256\nNEPOCHS = 100\ndataloader = ncycle(((selectdim(data, 2, ((i - 1) * B + 1):(min(i * B, size(data, 2)))),\n        selectdim(target, 2, ((i - 1) * B + 1):(min(i * B, size(data, 2)))))\n                     for i in 1:(size(data, 2) ÷ B)), NEPOCHS)\n\nhnn = HamiltonianNN(Chain(Dense(2 => 64, relu), Dense(64 => 1)); ad = AutoZygote())\nps, st = Lux.setup(Random.default_rng(), hnn)\nps_c = ps |> ComponentArray\n\nopt = OptimizationOptimisers.Adam(0.01f0)\n\nfunction loss_function(ps, data, target)\n    pred, st_ = hnn(data, ps, st)\n    return mean(abs2, pred .- target), pred\nend\n\nfunction callback(ps, loss, pred)\n    println(\"[Hamiltonian NN] Loss: \", loss)\n    return false\nend\n\nopt_func = OptimizationFunction((ps, _, data, target) -> loss_function(ps, data, target),\n    Optimization.AutoForwardDiff())\nopt_prob = OptimizationProblem(opt_func, ps_c)\n\nres = Optimization.solve(opt_prob, opt, dataloader; callback)\n\nps_trained = res.u\n\nmodel = NeuralHamiltonianDE(hnn, (0.0f0, 1.0f0), Tsit5(); save_everystep = false,\n    save_start = true, saveat = t)\n\npred = Array(first(model(data[:, 1], ps_trained, st)))\nplot(data[1, :], data[2, :]; lw = 4, label = \"Original\")\nplot!(pred[1, :], pred[2, :]; lw = 4, label = \"Predicted\")\nxlabel!(\"Position (q)\")\nylabel!(\"Momentum (p)\")","category":"page"},{"location":"examples/hamiltonian_nn/#Step-by-Step-Explanation","page":"Hamiltonian Neural Network","title":"Step by Step Explanation","text":"","category":"section"},{"location":"examples/hamiltonian_nn/#Data-Generation","page":"Hamiltonian Neural Network","title":"Data Generation","text":"","category":"section"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"The HNN predicts the gradients (dot q dot p) given (q p). Hence, we generate the pairs (q p) using the equations given at the top. Additionally, to supervise the training, we also generate the gradients. Next, we use Flux DataLoader for automatically batching our dataset.","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"using Lux, DiffEqFlux, OrdinaryDiffEq, Statistics, Plots, Zygote, ForwardDiff, Random,\n    ComponentArrays, Optimization, OptimizationOptimisers, IterTools\n\nt = range(0.0f0, 1.0f0; length = 1024)\nπ_32 = Float32(π)\nq_t = reshape(sin.(2π_32 * t), 1, :)\np_t = reshape(cos.(2π_32 * t), 1, :)\ndqdt = 2π_32 .* p_t\ndpdt = -2π_32 .* q_t\n\ndata = cat(q_t, p_t; dims = 1)\ntarget = cat(dqdt, dpdt; dims = 1)\nB = 256\nNEPOCHS = 100\ndataloader = ncycle(((selectdim(data, 2, ((i - 1) * B + 1):(min(i * B, size(data, 2)))),\n        selectdim(target, 2, ((i - 1) * B + 1):(min(i * B, size(data, 2)))))\n                     for i in 1:(size(data, 2) ÷ B)), NEPOCHS)","category":"page"},{"location":"examples/hamiltonian_nn/#Training-the-HamiltonianNN","page":"Hamiltonian Neural Network","title":"Training the HamiltonianNN","text":"","category":"section"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"We parameterize the HamiltonianNN with a small MultiLayered Perceptron. HNNs are trained by optimizing the gradients of the Neural Network. Zygote currently doesn't support nesting itself, so we will be using ForwardDiff in the training loop to compute the gradients of the HNN Layer for Optimization.","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"hnn = HamiltonianNN(Chain(Dense(2 => 64, relu), Dense(64 => 1)); ad = AutoZygote())\nps, st = Lux.setup(Random.default_rng(), hnn)\nps_c = ps |> ComponentArray\n\nopt = OptimizationOptimisers.Adam(0.01f0)\n\nfunction loss_function(ps, data, target)\n    pred, st_ = hnn(data, ps, st)\n    return mean(abs2, pred .- target), pred\nend\n\nfunction callback(ps, loss, pred)\n    println(\"[Hamiltonian NN] Loss: \", loss)\n    return false\nend\n\nopt_func = OptimizationFunction((ps, _, data, target) -> loss_function(ps, data, target),\n    Optimization.AutoZygote())\nopt_prob = OptimizationProblem(opt_func, ps_c)\n\nres = solve(opt_prob, opt, dataloader; callback)\n\nps_trained = res.u","category":"page"},{"location":"examples/hamiltonian_nn/#Solving-the-ODE-using-trained-HNN","page":"Hamiltonian Neural Network","title":"Solving the ODE using trained HNN","text":"","category":"section"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"In order to visualize the learned trajectories, we need to solve the ODE. We will use the NeuralHamiltonianDE layer, which is essentially a wrapper over HamiltonianNN layer, and solves the ODE.","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"model = NeuralHamiltonianDE(hnn, (0.0f0, 1.0f0), Tsit5(); save_everystep = false,\n    save_start = true, saveat = t)\n\npred = Array(first(model(data[:, 1], ps_trained, st)))\nplot(data[1, :], data[2, :]; lw = 4, label = \"Original\")\nplot!(pred[1, :], pred[2, :]; lw = 4, label = \"Predicted\")\nxlabel!(\"Position (q)\")\nylabel!(\"Momentum (p)\")","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"(Image: HNN Prediction)","category":"page"},{"location":"examples/hamiltonian_nn/#Expected-Output","page":"Hamiltonian Neural Network","title":"Expected Output","text":"","category":"section"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"Loss: 19.865715\nLoss: 18.196068\nLoss: 19.179213\nLoss: 19.58956\n⋮\nLoss: 0.02267044\nLoss: 0.019175647\nLoss: 0.02218909\nLoss: 0.018870523","category":"page"},{"location":"examples/hamiltonian_nn/#References","page":"Hamiltonian Neural Network","title":"References","text":"","category":"section"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"[1] Greydanus, Samuel, Misko Dzamba, and Jason Yosinski. \"Hamiltonian Neural Networks.\" Advances in Neural Information Processing Systems 32 (2019): 15379-15389.","category":"page"},{"location":"layers/HamiltonianNN/#Hamiltonian-Neural-Network","page":"Hamiltonian Neural Network Layer","title":"Hamiltonian Neural Network","text":"","category":"section"},{"location":"layers/HamiltonianNN/","page":"Hamiltonian Neural Network Layer","title":"Hamiltonian Neural Network Layer","text":"The following layer helps construct a neural network which allows learning dynamics and conservation laws by approximating the hamiltonian of a system.","category":"page"},{"location":"layers/HamiltonianNN/","page":"Hamiltonian Neural Network Layer","title":"Hamiltonian Neural Network Layer","text":"HamiltonianNN\nNeuralHamiltonianDE","category":"page"},{"location":"layers/HamiltonianNN/#DiffEqFlux.HamiltonianNN","page":"Hamiltonian Neural Network Layer","title":"DiffEqFlux.HamiltonianNN","text":"HamiltonianNN(model; ad = AutoForwardDiff())\n\nConstructs a Hamiltonian Neural Network [1]. This neural network is useful for learning symmetries and conservation laws by supervision on the gradients of the trajectories. It takes as input a concatenated vector of length 2n containing the position (of size n) and momentum (of size n) of the particles. It then returns the time derivatives for position and momentum.\n\nnote: Note\nThis doesn't solve the Hamiltonian Problem. Use NeuralHamiltonianDE for such applications.\n\nArguments:\n\nmodel: A Flux.Chain or Lux.AbstractExplicitLayer neural network that returns the Hamiltonian of the system.\nad: The autodiff framework to be used for the internal Hamiltonian computation. The default is AutoForwardDiff()\n\nnote: Note\nIf training with Zygote, ensure that the chunksize for AutoForwardDiff is set to nothing.\n\nReferences:\n\n[1] Greydanus, Samuel, Misko Dzamba, and Jason Yosinski. \"Hamiltonian Neural Networks.\" Advances in Neural Information Processing Systems 32 (2019): 15379-15389.\n\n\n\n\n\n","category":"type"},{"location":"layers/HamiltonianNN/#DiffEqFlux.NeuralHamiltonianDE","page":"Hamiltonian Neural Network Layer","title":"DiffEqFlux.NeuralHamiltonianDE","text":"NeuralHamiltonianDE(model, tspan, args...; kwargs...)\n\nConstructs a Neural Hamiltonian DE Layer for solving Hamiltonian Problems parameterized by a Neural Network HamiltonianNN.\n\nArguments:\n\nmodel: A Flux.Chain, Lux.AbstractExplicitLayer, or Hamiltonian Neural Network that predicts the Hamiltonian of the system.\ntspan: The timespan to be solved on.\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"layers/BasisLayers/#Classical-Basis-Layers","page":"Classical Basis Layers","title":"Classical Basis Layers","text":"","category":"section"},{"location":"layers/BasisLayers/","page":"Classical Basis Layers","title":"Classical Basis Layers","text":"The following basis are helper functions for easily building arrays of the form [f0(x), ..., f{n-1}(x)], where f is the corresponding function of the basis (e.g, Chebyshev Polynomials, Legendre Polynomials, etc.)","category":"page"},{"location":"layers/BasisLayers/","page":"Classical Basis Layers","title":"Classical Basis Layers","text":"ChebyshevBasis\nSinBasis\nCosBasis\nFourierBasis\nLegendreBasis\nPolynomialBasis","category":"page"},{"location":"layers/BasisLayers/#DiffEqFlux.ChebyshevBasis","page":"Classical Basis Layers","title":"DiffEqFlux.ChebyshevBasis","text":"ChebyshevBasis(n)\n\nConstructs a Chebyshev basis of the form [T{0}(x), T{1}(x), ..., T{n-1}(x)] where Tj(.) is the j-th Chebyshev polynomial of the first kind.\n\nArguments:\n\nn: number of terms in the polynomial expansion.\n\n\n\n\n\n","category":"function"},{"location":"layers/BasisLayers/#DiffEqFlux.SinBasis","page":"Classical Basis Layers","title":"DiffEqFlux.SinBasis","text":"SinBasis(n)\n\nConstructs a sine basis of the form [sin(x), sin(2x), ..., sin(nx)].\n\nArguments:\n\nn: number of terms in the sine expansion.\n\n\n\n\n\n","category":"function"},{"location":"layers/BasisLayers/#DiffEqFlux.CosBasis","page":"Classical Basis Layers","title":"DiffEqFlux.CosBasis","text":"CosBasis(n)\n\nConstructs a cosine basis of the form [cos(x), cos(2x), ..., cos(nx)].\n\nArguments:\n\nn: number of terms in the cosine expansion.\n\n\n\n\n\n","category":"function"},{"location":"layers/BasisLayers/#DiffEqFlux.FourierBasis","page":"Classical Basis Layers","title":"DiffEqFlux.FourierBasis","text":"FourierBasis(n)\n\nConstructs a Fourier basis of the form Fj(x) = j is even ? cos((j÷2)x) : sin((j÷2)x) => [F0(x), F1(x), ..., Fn(x)].\n\nArguments:\n\nn: number of terms in the Fourier expansion.\n\n\n\n\n\n","category":"function"},{"location":"layers/BasisLayers/#DiffEqFlux.LegendreBasis","page":"Classical Basis Layers","title":"DiffEqFlux.LegendreBasis","text":"LegendreBasis(n)\n\nConstructs a Legendre basis of the form [P{0}(x), P{1}(x), ..., P{n-1}(x)] where Pj(.) is the j-th Legendre polynomial.\n\nArguments:\n\nn: number of terms in the polynomial expansion.\n\n\n\n\n\n","category":"function"},{"location":"layers/BasisLayers/#DiffEqFlux.PolynomialBasis","page":"Classical Basis Layers","title":"DiffEqFlux.PolynomialBasis","text":"PolynomialBasis(n)\n\nConstructs a Polynomial basis of the form [1, x, ..., x^(n-1)].\n\nArguments:\n\nn: number of terms in the polynomial expansion.\n\n\n\n\n\n","category":"function"},{"location":"examples/augmented_neural_ode/#Augmented-Neural-Ordinary-Differential-Equations","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#Copy-Pasteable-Code","page":"Augmented Neural Ordinary Differential Equations","title":"Copy-Pasteable Code","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"using DiffEqFlux, OrdinaryDiffEq, Statistics, LinearAlgebra, Plots, LuxCUDA, Random\nusing MLUtils, ComponentArrays\nusing Optimization, OptimizationOptimisers, IterTools\n\nconst cdev = cpu_device()\nconst gdev = gpu_device()\n\nfunction random_point_in_sphere(dim, min_radius, max_radius)\n    distance = (max_radius - min_radius) .* (rand(Float32, 1) .^ (1.0f0 / dim)) .+\n               min_radius\n    direction = randn(Float32, dim)\n    unit_direction = direction ./ norm(direction)\n    return distance .* unit_direction\nend\n\nfunction concentric_sphere(dim, inner_radius_range, outer_radius_range,\n        num_samples_inner, num_samples_outer; batch_size = 64)\n    data = []\n    labels = []\n    for _ in 1:num_samples_inner\n        push!(data, reshape(random_point_in_sphere(dim, inner_radius_range...), :, 1))\n        push!(labels, ones(1, 1))\n    end\n    for _ in 1:num_samples_outer\n        push!(data, reshape(random_point_in_sphere(dim, outer_radius_range...), :, 1))\n        push!(labels, -ones(1, 1))\n    end\n    data = cat(data...; dims = 2)\n    labels = cat(labels...; dims = 2)\n    return DataLoader((data |> gdev, labels |> gdev); batchsize = batch_size,\n        shuffle = true, partial = false)\nend\n\ndiffeqarray_to_array(x) = reshape(gdev(x), size(x)[1:2])\n\nfunction construct_model(out_dim, input_dim, hidden_dim, augment_dim)\n    input_dim = input_dim + augment_dim\n    node = NeuralODE(Chain(Dense(input_dim, hidden_dim, relu),\n            Dense(hidden_dim, hidden_dim, relu),\n            Dense(hidden_dim, input_dim)), (0.0f0, 1.0f0), Tsit5(); save_everystep = false,\n        reltol = 1.0f-3, abstol = 1.0f-3, save_start = false)\n    node = augment_dim == 0 ? node : AugmentedNDELayer(node, augment_dim)\n    model = Chain(node, diffeqarray_to_array, Dense(input_dim, out_dim))\n    ps, st = Lux.setup(Random.default_rng(), model)\n    return model, ps |> gdev, st |> gdev\nend\n\nfunction plot_contour(model, ps, st, npoints = 300)\n    grid_points = zeros(Float32, 2, npoints^2)\n    idx = 1\n    x = range(-4.0f0, 4.0f0; length = npoints)\n    y = range(-4.0f0, 4.0f0; length = npoints)\n    for x1 in x, x2 in y\n        grid_points[:, idx] .= [x1, x2]\n        idx += 1\n    end\n    sol = reshape(model(grid_points |> gdev, ps, st)[1], npoints, npoints) |> cdev\n\n    return contour(x, y, sol; fill = true, linewidth = 0.0)\nend\n\nloss_node(model, x, y, ps, st) = mean((first(model(x, ps, st)) .- y) .^ 2)\n\nprintln(\"Generating Dataset\")\n\ndataloader = concentric_sphere(2, (0.0f0, 2.0f0), (3.0f0, 4.0f0), 2000, 2000;\n    batch_size = 256)\n\niter = 0\ncb = function (ps, l)\n    global iter\n    iter += 1\n    if iter % 10 == 0\n        @info \"Augmented Neural ODE\" iter=iter loss=l\n    end\n    return false\nend\n\nmodel, ps, st = construct_model(1, 2, 64, 0)\nopt = Adam(0.005)\n\nloss_node(model, dataloader.data[1], dataloader.data[2], ps, st)\n\nprintln(\"Training Neural ODE\")\n\noptfunc = OptimizationFunction((x, p, data, target) -> loss_node(model, data, target, x, st),\n    Optimization.AutoZygote())\noptprob = OptimizationProblem(optfunc, ComponentArray(ps |> cdev) |> gdev)\nres = solve(optprob, opt, IterTools.ncycle(dataloader, 5); callback = cb)\n\nplt_node = plot_contour(model, res.u, st)\n\nmodel, ps, st = construct_model(1, 2, 64, 1)\nopt = Adam(0.005)\n\nprintln()\nprintln(\"Training Augmented Neural ODE\")\n\noptfunc = OptimizationFunction((x, p, data, target) -> loss_node(model, data, target, x, st),\n    Optimization.AutoZygote())\noptprob = OptimizationProblem(optfunc, ComponentArray(ps |> cdev) |> gdev)\nres = solve(optprob, opt, IterTools.ncycle(dataloader, 5); callback = cb)\n\nplt_node = plot_contour(model, res.u, st)","category":"page"},{"location":"examples/augmented_neural_ode/#Step-by-Step-Explanation","page":"Augmented Neural Ordinary Differential Equations","title":"Step-by-Step Explanation","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#Loading-required-packages","page":"Augmented Neural Ordinary Differential Equations","title":"Loading required packages","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"using DiffEqFlux, OrdinaryDiffEq, Statistics, LinearAlgebra, Plots, LuxCUDA, Random\nusing MLUtils, ComponentArrays\nusing Optimization, OptimizationOptimisers, IterTools\n\nconst cdev = cpu_device()\nconst gdev = gpu_device()","category":"page"},{"location":"examples/augmented_neural_ode/#Generating-a-toy-dataset","page":"Augmented Neural Ordinary Differential Equations","title":"Generating a toy dataset","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"In this example, we will be using data sampled uniformly in two concentric circles and then train our Neural ODEs to do regression on that values. We assign 1 to any point which lies inside the inner circle, and -1 to any point which lies between the inner and outer circle. Our first function random_point_in_sphere samples points uniformly between 2 concentric circles/spheres of radii min_radius and max_radius respectively.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"function random_point_in_sphere(dim, min_radius, max_radius)\n    distance = (max_radius - min_radius) .* (rand(Float32, 1) .^ (1.0f0 / dim)) .+\n               min_radius\n    direction = randn(Float32, dim)\n    unit_direction = direction ./ norm(direction)\n    return distance .* unit_direction\nend","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Next, we will construct a dataset of these points and use Flux's DataLoader to automatically minibatch and shuffle the data.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"function concentric_sphere(dim, inner_radius_range, outer_radius_range,\n        num_samples_inner, num_samples_outer; batch_size = 64)\n    data = []\n    labels = []\n    for _ in 1:num_samples_inner\n        push!(data, reshape(random_point_in_sphere(dim, inner_radius_range...), :, 1))\n        push!(labels, ones(1, 1))\n    end\n    for _ in 1:num_samples_outer\n        push!(data, reshape(random_point_in_sphere(dim, outer_radius_range...), :, 1))\n        push!(labels, -ones(1, 1))\n    end\n    data = cat(data...; dims = 2)\n    labels = cat(labels...; dims = 2)\n    return DataLoader((data |> gdev, labels |> gdev); batchsize = batch_size,\n        shuffle = true, partial = false)\nend","category":"page"},{"location":"examples/augmented_neural_ode/#Models","page":"Augmented Neural Ordinary Differential Equations","title":"Models","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"We consider 2 models in this tutorial. The first is a simple Neural ODE which is described in detail in this tutorial. The other one is an Augmented Neural ODE [1]. The idea behind this layer is very simple. It augments the input to the Neural DE Layer by appending zeros. So in order to use any arbitrary DE Layer in combination with this layer, simply assume that the input to the DE Layer is of size size(x, 1) + augment_dim instead of size(x, 1) and construct that layer accordingly.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"In order to run the models on Flux.gpu, we need to manually transfer the models to Flux.gpu. First one is the network predicting the derivatives inside the Neural ODE and the other one is the last layer in the Chain.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"diffeqarray_to_array(x) = reshape(gdev(x), size(x)[1:2])\n\nfunction construct_model(out_dim, input_dim, hidden_dim, augment_dim)\n    input_dim = input_dim + augment_dim\n    node = NeuralODE(Chain(Dense(input_dim, hidden_dim, relu),\n            Dense(hidden_dim, hidden_dim, relu),\n            Dense(hidden_dim, input_dim)), (0.0f0, 1.0f0), Tsit5(); save_everystep = false,\n        reltol = 1.0f-3, abstol = 1.0f-3, save_start = false)\n    node = augment_dim == 0 ? node : AugmentedNDELayer(node, augment_dim)\n    model = Chain(node, diffeqarray_to_array, Dense(input_dim, out_dim))\n    ps, st = Lux.setup(Random.default_rng(), model)\n    return model, ps |> gdev, st |> gdev\nend","category":"page"},{"location":"examples/augmented_neural_ode/#Plotting-the-Results","page":"Augmented Neural Ordinary Differential Equations","title":"Plotting the Results","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Here, we define a utility to plot our model regression results as a heatmap.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"function plot_contour(model, ps, st, npoints = 300)\n    grid_points = zeros(Float32, 2, npoints^2)\n    idx = 1\n    x = range(-4.0f0, 4.0f0; length = npoints)\n    y = range(-4.0f0, 4.0f0; length = npoints)\n    for x1 in x, x2 in y\n        grid_points[:, idx] .= [x1, x2]\n        idx += 1\n    end\n    sol = reshape(model(grid_points |> gdev, ps, st)[1], npoints, npoints) |> cdev\n\n    return contour(x, y, sol; fill = true, linewidth = 0.0)\nend","category":"page"},{"location":"examples/augmented_neural_ode/#Training-Parameters","page":"Augmented Neural Ordinary Differential Equations","title":"Training Parameters","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#Loss-Functions","page":"Augmented Neural Ordinary Differential Equations","title":"Loss Functions","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"We use the L2 distance between the model prediction model(x) and the actual prediction y as the optimization objective.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"loss_node(model, x, y, ps, st) = mean((first(model(x, ps, st)) .- y) .^ 2)","category":"page"},{"location":"examples/augmented_neural_ode/#Dataset","page":"Augmented Neural Ordinary Differential Equations","title":"Dataset","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Next, we generate the dataset. We restrict ourselves to 2 dimensions as it is easy to visualize. We sample a total of 4000 data points.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"dataloader = concentric_sphere(2, (0.0f0, 2.0f0), (3.0f0, 4.0f0), 2000, 2000;\n    batch_size = 256)","category":"page"},{"location":"examples/augmented_neural_ode/#Callback-Function","page":"Augmented Neural Ordinary Differential Equations","title":"Callback Function","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Additionally, we define a callback function which displays the total loss at specific intervals.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"iter = 0\ncb = function (ps, l)\n    global iter\n    iter += 1\n    if iter % 10 == 0\n        @info \"Augmented Neural ODE\" iter=iter loss=l\n    end\n    return false\nend","category":"page"},{"location":"examples/augmented_neural_ode/#Optimizer","page":"Augmented Neural Ordinary Differential Equations","title":"Optimizer","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"We use Adam as the optimizer with a learning rate of 0.005","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"opt = Adam(5.0f-3)","category":"page"},{"location":"examples/augmented_neural_ode/#Training-the-Neural-ODE","page":"Augmented Neural Ordinary Differential Equations","title":"Training the Neural ODE","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"To train our neural ode model, we need to pass the appropriate learnable parameters, parameters which are returned by the construct_models function. It is simply the node.p vector. We then train our model for 20 epochs.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"model, ps, st = construct_model(1, 2, 64, 0)\n\noptfunc = OptimizationFunction((x, p, data, target) -> loss_node(model, data, target, x, st),\n    Optimization.AutoZygote())\noptprob = OptimizationProblem(optfunc, ComponentArray(ps |> cdev) |> gdev)\nres = solve(optprob, opt, IterTools.ncycle(dataloader, 5); callback = cb)","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Here is what the contour plot should look for Neural ODE. Notice that the regression is not perfect due to the thin artifact which connects the circles.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"(Image: node)","category":"page"},{"location":"examples/augmented_neural_ode/#Training-the-Augmented-Neural-ODE","page":"Augmented Neural Ordinary Differential Equations","title":"Training the Augmented Neural ODE","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Our training configuration will be the same as that of Neural ODE. Only in this case, we have augmented the input with a single zero. This makes the problem 3-dimensional, and as such it is possible to find a function which can be expressed by the neural ode. For more details and proofs, please refer to [1].","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"model, ps, st = construct_model(1, 2, 64, 1)\n\noptfunc = OptimizationFunction((x, p, data, target) -> loss_node(model, data, target, x, st),\n    Optimization.AutoZygote())\noptprob = OptimizationProblem(optfunc, ComponentArray(ps |> cdev) |> gdev)\nres = solve(optprob, opt, IterTools.ncycle(dataloader, 5); callback = cb)","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"For the augmented Neural ODE we notice that the artifact is gone.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"(Image: anode)","category":"page"},{"location":"examples/augmented_neural_ode/#Expected-Output","page":"Augmented Neural Ordinary Differential Equations","title":"Expected Output","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Generating Dataset\n┌ Info: Augmented Neural ODE\n│   iter = 10\n└   loss = 1.3382126f0\n┌ Info: Augmented Neural ODE\n│   iter = 20\n└   loss = 0.7405951f0\n┌ Info: Augmented Neural ODE\n│   iter = 30\n└   loss = 0.65393615f0\n┌ Info: Augmented Neural ODE\n│   iter = 40\n└   loss = 0.6115348f0\n┌ Info: Augmented Neural ODE\n│   iter = 50\n└   loss = 0.5469544f0\n┌ Info: Augmented Neural ODE\n│   iter = 60\n└   loss = 0.61832863f0\n┌ Info: Augmented Neural ODE\n│   iter = 70\n└   loss = 0.45164242f0\n\nTraining Augmented Neural ODE\n┌ Info: Augmented Neural ODE\n│   iter = 80\n└   loss = 2.5972328f0\n┌ Info: Augmented Neural ODE\n│   iter = 90\n└   loss = 0.79345906f0\n┌ Info: Augmented Neural ODE\n│   iter = 100\n└   loss = 0.6131873f0\n┌ Info: Augmented Neural ODE\n│   iter = 110\n└   loss = 0.36244678f0\n┌ Info: Augmented Neural ODE\n│   iter = 120\n└   loss = 0.14108367f0\n┌ Info: Augmented Neural ODE\n│   iter = 130\n└   loss = 0.09875094f0\n┌ Info: Augmented Neural ODE\n│   iter = 140\n└   loss = 0.060682703f0\n┌ Info: Augmented Neural ODE\n│   iter = 150\n└   loss = 0.050104875f0","category":"page"},{"location":"examples/augmented_neural_ode/#References","page":"Augmented Neural Ordinary Differential Equations","title":"References","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"[1] Dupont, Emilien, Arnaud Doucet, and Yee Whye Teh. \"Augmented neural ODEs.\" In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pp. 3140-3150. 2019.","category":"page"},{"location":"utilities/MultipleShooting/#Multiple-Shooting-Functionality","page":"Multiple Shooting Functionality","title":"Multiple Shooting Functionality","text":"","category":"section"},{"location":"utilities/MultipleShooting/","page":"Multiple Shooting Functionality","title":"Multiple Shooting Functionality","text":"note: Note\nThe form of multiple shooting found here is a specialized form for implicit layer deep learning (known as data shooting) which assumes full observability of the underlying dynamics and lack of noise. For a more general implementation of multiple shooting, see the JuliaSimModelOptimizer. For an implementation more directly tied to parameter estimation against data, see DiffEqParamEstim.jl.","category":"page"},{"location":"utilities/MultipleShooting/","page":"Multiple Shooting Functionality","title":"Multiple Shooting Functionality","text":"multiple_shoot\nDiffEqFlux.group_ranges","category":"page"},{"location":"utilities/MultipleShooting/#DiffEqFlux.multiple_shoot","page":"Multiple Shooting Functionality","title":"DiffEqFlux.multiple_shoot","text":"multiple_shoot(p, ode_data, tsteps, prob, loss_function,\n    [continuity_loss = _default_continuity_loss], solver, group_size;\n    continuity_term = 100, kwargs...)\n\nReturns a total loss after trying a 'Direct multiple shooting' on ODE data and an array of predictions from each of the groups (smaller intervals). In Direct Multiple Shooting, the Neural Network divides the interval into smaller intervals and solves for them separately. The default continuity term is 100, implying any losses arising from the non-continuity of 2 different groups will be scaled by 100.\n\nArguments:\n\np: The parameters of the Neural Network to be trained.\node_data: Original Data to be modelled.\ntsteps: Timesteps on which ode_data was calculated.\nprob: ODE problem that the Neural Network attempts to solve.\nloss_function: Any arbitrary function to calculate loss.\ncontinuity_loss: Function that takes states hatu_end of group k and u_0 of group k+1 as input and calculates prediction continuity loss between them. If no custom continuity_loss is specified, sum(abs, û_end - u_0) is used.\nsolver: ODE Solver algorithm.\ngroup_size: The group size achieved after splitting the ode_data into equal sizes.\ncontinuity_term: Weight term to ensure continuity of predictions throughout different groups.\nkwargs: Additional arguments splatted to the ODE solver. Refer to the Local Sensitivity Analysis and Common Solver Arguments documentation for more details.\n\nnote: Note\nThe parameter 'continuity_term' should be a relatively big number to enforce a large penalty whenever the last point of any group doesn't coincide with the first point of next group.\n\n\n\n\n\nmultiple_shoot(p, ode_data, tsteps, ensembleprob, ensemblealg, loss_function,\n    [continuity_loss = _default_continuity_loss], solver, group_size;\n    continuity_term = 100, kwargs...)\n\nReturns a total loss after trying a 'Direct multiple shooting' on ODE data and an array of predictions from each of the groups (smaller intervals). In Direct Multiple Shooting, the Neural Network divides the interval into smaller intervals and solves for them separately. The default continuity term is 100, implying any losses arising from the non-continuity of 2 different groups will be scaled by 100.\n\nArguments:\n\np: The parameters of the Neural Network to be trained.\node_data: Original Data to be modelled.\ntsteps: Timesteps on which ode_data was calculated.\nensemble_prob: Ensemble problem that the Neural Network attempts to solve.\nensemble_alg: Ensemble algorithm, e.g. EnsembleThreads()\nprob: ODE problem that the Neural Network attempts to solve.\nloss_function: Any arbitrary function to calculate loss.\ncontinuity_loss: Function that takes states hatu_end of group k and\n\nu_0 of group k+1 as input and calculates prediction continuity loss between them. If no custom continuity_loss is specified, sum(abs, û_end - u_0) is used.\n\nsolver: ODE Solver algorithm.\ngroup_size: The group size achieved after splitting the ode_data into equal sizes.\ncontinuity_term: Weight term to ensure continuity of predictions throughout\n\ndifferent groups.\n\nkwargs: Additional arguments splatted to the ODE solver. Refer to the\n\nLocal Sensitivity Analysis and Common Solver Arguments documentation for more details.\n\nnote: Note\nThe parameter 'continuity_term' should be a relatively big number to enforce a large penalty whenever the last point of any group doesn't coincide with the first point of next group.\n\n\n\n\n\n","category":"function"},{"location":"utilities/MultipleShooting/#DiffEqFlux.group_ranges","page":"Multiple Shooting Functionality","title":"DiffEqFlux.group_ranges","text":"group_ranges(datasize, groupsize)\n\nGet ranges that partition data of length datasize in groups of groupsize observations. If the data isn't perfectly dividable by groupsize, the last group contains the reminding observations.\n\nArguments:\n\ndatasize: amount of data points to be partitioned\ngroupsize: maximum amount of observations in each group\n\nExample:\n\njulia> group_ranges(10, 5)\n3-element Vector{UnitRange{Int64}}:\n 1:5\n 5:9\n 9:10\n\n\n\n\n\n","category":"function"},{"location":"examples/neural_ode_weather_forecast/#Weather-forecasting-with-neural-ODEs","page":"Weather forecasting with neural ODEs","title":"Weather forecasting with neural ODEs","text":"","category":"section"},{"location":"examples/neural_ode_weather_forecast/","page":"Weather forecasting with neural ODEs","title":"Weather forecasting with neural ODEs","text":"In this example we are going to apply neural ODEs to a multidimensional weather dataset and use it for weather forecasting. This example is adapted from Forecasting the weather with neural ODEs - Sebatian Callh personal blog.","category":"page"},{"location":"examples/neural_ode_weather_forecast/#The-data","page":"Weather forecasting with neural ODEs","title":"The data","text":"","category":"section"},{"location":"examples/neural_ode_weather_forecast/","page":"Weather forecasting with neural ODEs","title":"Weather forecasting with neural ODEs","text":"The data is a four-dimensional dataset of daily temperature, humidity, wind speed and pressure measured over four years in the city Delhi. Let us download and plot it.","category":"page"},{"location":"examples/neural_ode_weather_forecast/","page":"Weather forecasting with neural ODEs","title":"Weather forecasting with neural ODEs","text":"using Random, Dates, Optimization, ComponentArrays, Lux, OptimizationOptimisers, DiffEqFlux,\n    OrdinaryDiffEq, CSV, DataFrames, Dates, Statistics, Plots, DataDeps\n\nfunction download_data(data_url = \"https://raw.githubusercontent.com/SebastianCallh/neural-ode-weather-forecast/master/data/\",\n        data_local_path = \"./delhi\")\n    function load(file_name)\n        data_dep = DataDep(\"delhi/train\", \"\", \"$data_url/$file_name\")\n        Base.download(data_dep, data_local_path; i_accept_the_terms_of_use = true)\n        CSV.read(joinpath(data_local_path, file_name), DataFrame)\n    end\n\n    train_df = load(\"DailyDelhiClimateTrain.csv\")\n    test_df = load(\"DailyDelhiClimateTest.csv\")\n    return vcat(train_df, test_df)\nend\n\ndf = download_data()","category":"page"},{"location":"examples/neural_ode_weather_forecast/","page":"Weather forecasting with neural ODEs","title":"Weather forecasting with neural ODEs","text":"FEATURES = [:meantemp, :humidity, :wind_speed, :meanpressure]\nUNITS = [\"Celsius\", \"g/m³ of water\", \"km/h\", \"hPa\"]\nFEATURE_NAMES = [\"Mean temperature\", \"Humidity\", \"Wind speed\", \"Mean pressure\"]\n\nfunction plot_data(df)\n    plots = map(enumerate(zip(FEATURES, FEATURE_NAMES, UNITS))) do (i, (f, n, u))\n        plot(df[:, :date], df[:, f]; title = n, label = nothing,\n            ylabel = u, size = (800, 600), color = i)\n    end\n\n    n = length(plots)\n    plot(plots...; layout = (Int(n / 2), Int(n / 2)))\nend\n\nplot_data(df)","category":"page"},{"location":"examples/neural_ode_weather_forecast/","page":"Weather forecasting with neural ODEs","title":"Weather forecasting with neural ODEs","text":"The data show clear annual behaviour (it is difficult to see for pressure due to wild measurement errors but the pattern is there). It is concievable that this system can be described with an ODE, but which? Let us use an network to learn the dynamics from the dataset. Training neural networks is easier with standardised data so we will compute standardised features before training. Finally, we take the first 20 days for training and the rest for testing.","category":"page"},{"location":"examples/neural_ode_weather_forecast/","page":"Weather forecasting with neural ODEs","title":"Weather forecasting with neural ODEs","text":"function standardize(x)\n    μ = mean(x; dims = 2)\n    σ = std(x; dims = 2)\n    z = (x .- μ) ./ σ\n    return z, μ, σ\nend\n\nfunction featurize(raw_df, num_train = 20)\n    raw_df.year = Float64.(year.(raw_df.date))\n    raw_df.month = Float64.(month.(raw_df.date))\n    df = combine(groupby(raw_df, [:year, :month]),\n        :date => (d -> mean(year.(d)) .+ mean(month.(d)) ./ 12),\n        :meantemp => mean,\n        :humidity => mean,\n        :wind_speed => mean,\n        :meanpressure => mean;\n        renamecols = false)\n    @show size(df)\n    t_and_y(df) = df.date', Matrix(select(df, FEATURES))'\n    t_train, y_train = t_and_y(df[1:num_train, :])\n    t_test, y_test = t_and_y(df[(num_train + 1):end, :])\n    t_train, t_mean, t_scale = standardize(t_train)\n    y_train, y_mean, y_scale = standardize(y_train)\n    t_test = (t_test .- t_mean) ./ t_scale\n    y_test = (y_test .- y_mean) ./ y_scale\n\n    return (vec(t_train), y_train,\n        vec(t_test), y_test,\n        (t_mean, t_scale),\n        (y_mean, y_scale))\nend\n\nfunction plot_features(t_train, y_train, t_test, y_test)\n    plt_split = plot(reshape(t_train, :), y_train';\n        linewidth = 3, colors = 1:4,\n        xlabel = \"Normalized time\",\n        ylabel = \"Normalized values\",\n        label = nothing,\n        title = \"Features\")\n    plot!(plt_split, reshape(t_test, :), y_test';\n        linewidth = 3, linestyle = :dash,\n        color = [1 2 3 4], label = nothing)\n\n    plot!(plt_split, [0], [0]; linewidth = 0,\n        label = \"Train\", color = 1)\n    plot!(plt_split, [0], [0]; linewidth = 0,\n        linestyle = :dash, label = \"Test\",\n        color = 1,\n        ylims = (-5, 5))\nend\n\n(t_train,\ny_train,\nt_test,\ny_test,\n(t_mean, t_scale),\n(y_mean, y_scale)) = featurize(df)\nplot_features(t_train, y_train, t_test, y_test)","category":"page"},{"location":"examples/neural_ode_weather_forecast/","page":"Weather forecasting with neural ODEs","title":"Weather forecasting with neural ODEs","text":"The dataset is now centered around 0 with a standard deviation of 1. We will ignore the extreme pressure measurements for simplicity. Since they are in the test split they won't impact training anyway. We are now ready to construct and train our model! To avoid local minimas we will train iteratively with increasing amounts of data.","category":"page"},{"location":"examples/neural_ode_weather_forecast/","page":"Weather forecasting with neural ODEs","title":"Weather forecasting with neural ODEs","text":"function neural_ode(t, data_dim)\n    f = Chain(Dense(data_dim => 64, swish), Dense(64 => 32, swish), Dense(32 => data_dim))\n\n    node = NeuralODE(f, extrema(t), Tsit5(); saveat = t,\n        abstol = 1e-9, reltol = 1e-9)\n\n    rng = Random.default_rng()\n    p, state = Lux.setup(rng, f)\n\n    return node, ComponentArray(p), state\nend\n\nfunction train_one_round(node, p, state, y, opt, maxiters, rng, y0 = y[:, 1]; kwargs...)\n    predict(p) = Array(node(y0, p, state)[1])\n    loss(p) = sum(abs2, predict(p) .- y)\n\n    adtype = Optimization.AutoZygote()\n    optf = OptimizationFunction((p, _) -> loss(p), adtype)\n    optprob = OptimizationProblem(optf, p)\n    res = solve(optprob, opt; maxiters = maxiters, kwargs...)\n    res.minimizer, state\nend\n\nfunction train(t, y, obs_grid, maxiters, lr, rng, p = nothing, state = nothing; kwargs...)\n    log_results(ps, losses) = (p, loss) -> begin\n        push!(ps, copy(p))\n        push!(losses, loss)\n        false\n    end\n\n    ps, losses = ComponentArray[], Float32[]\n    for k in obs_grid\n        node, p_new, state_new = neural_ode(t, size(y, 1))\n        p === nothing && (p = p_new)\n        state === nothing && (state = state_new)\n\n        p, state = train_one_round(node, p, state, y, AdamW(lr), maxiters, rng;\n            callback = log_results(ps, losses), kwargs...)\n    end\n    ps, state, losses\nend\n\nrng = MersenneTwister(123)\nobs_grid = 4:4:length(t_train) # we train on an increasing amount of the first k obs\nmaxiters = 150\nlr = 5e-3\nps, state, losses = train(t_train, y_train, obs_grid, maxiters, lr, rng; progress = true);","category":"page"},{"location":"examples/neural_ode_weather_forecast/","page":"Weather forecasting with neural ODEs","title":"Weather forecasting with neural ODEs","text":"We can now animate the training to get a better understanding of the fit.","category":"page"},{"location":"examples/neural_ode_weather_forecast/","page":"Weather forecasting with neural ODEs","title":"Weather forecasting with neural ODEs","text":"predict(y0, t, p, state) = begin\n    node, _, _ = neural_ode(t, length(y0))\n    Array(node(y0, p, state)[1])\nend\n\nfunction plot_pred(t_train, y_train, t_grid, rescale_t, rescale_y, num_iters, p, state,\n        loss, y0 = y_train[:, 1])\n    y_pred = predict(y0, t_grid, p, state)\n    return plot_result(rescale_t(t_train), rescale_y(y_train), rescale_t(t_grid),\n        rescale_y(y_pred), loss, num_iters)\nend\n\nfunction plot_pred(t, y, y_pred)\n    plt = Plots.scatter(t, y; label = \"Observation\")\n    Plots.plot!(plt, t, y_pred; label = \"Prediction\")\nend\n\nfunction plot_pred(t, y, t_pred, y_pred; kwargs...)\n    plot_params = zip(eachrow(y), eachrow(y_pred), FEATURE_NAMES, UNITS)\n    map(enumerate(plot_params)) do (i, (yᵢ, ŷᵢ, name, unit))\n        plt = Plots.plot(t_pred, ŷᵢ; label = \"Prediction\", color = i, linewidth = 3,\n            legend = nothing, title = name, kwargs...)\n        Plots.scatter!(plt, t, yᵢ; label = \"Observation\", xlabel = \"Time\", ylabel = unit,\n            markersize = 5, color = i)\n    end\nend\n\nfunction plot_result(t, y, t_pred, y_pred, loss, num_iters; kwargs...)\n    plts_preds = plot_pred(t, y, t_pred, y_pred; kwargs...)\n    plot!(plts_preds[1]; ylim = (10, 40), legend = (0.65, 1.0))\n    plot!(plts_preds[2]; ylim = (20, 100))\n    plot!(plts_preds[3]; ylim = (2, 12))\n    plot!(plts_preds[4]; ylim = (990, 1025))\n\n    p_loss = Plots.plot(loss; label = nothing, linewidth = 3,\n        title = \"Loss\", xlabel = \"Iterations\", xlim = (0, num_iters))\n    plots = [plts_preds..., p_loss]\n    plot(plots...; layout = grid(length(plots), 1), size = (900, 900))\nend\n\nfunction animate_training(plot_frame, t_train, y_train, ps, losses, obs_grid;\n        pause_for = 300)\n    obs_count = Dict(i - 1 => n for (i, n) in enumerate(obs_grid))\n    is = [min(i, length(losses)) for i in 2:(length(losses) + pause_for)]\n    @animate for i in is\n        stage = Int(floor((i - 1) / length(losses) * length(obs_grid)))\n        k = obs_count[stage]\n        plot_frame(t_train[1:k], y_train[:, 1:k], ps[i], losses[1:i])\n    end every 2\nend\n\nnum_iters = length(losses)\nt_train_grid = collect(range(extrema(t_train)...; length = 500))\nrescale_t(x) = t_scale .* x .+ t_mean\nrescale_y(x) = y_scale .* x .+ y_mean\nfunction plot_frame(t, y, p, loss)\n    plot_pred(t, y, t_train_grid, rescale_t, rescale_y, num_iters, p, state, loss)\nend\nanim = animate_training(plot_frame, t_train, y_train, ps, losses, obs_grid);\ngif(anim, \"node_weather_forecast_training.gif\")","category":"page"},{"location":"examples/neural_ode_weather_forecast/","page":"Weather forecasting with neural ODEs","title":"Weather forecasting with neural ODEs","text":"Looks good! But how well does the model forecast?","category":"page"},{"location":"examples/neural_ode_weather_forecast/","page":"Weather forecasting with neural ODEs","title":"Weather forecasting with neural ODEs","text":"function plot_extrapolation(t_train, y_train, t_test, y_test, t̂, ŷ)\n    plts = plot_pred(t_train, y_train, t̂, ŷ)\n    for (i, (plt, y)) in enumerate(zip(plts, eachrow(y_test)))\n        scatter!(plt, t_test, y; color = i, markerstrokecolor = :white,\n            label = \"Test observation\")\n    end\n\n    plot!(plts[1]; ylim = (10, 40), legend = :topleft)\n    plot!(plts[2]; ylim = (20, 100))\n    plot!(plts[3]; ylim = (2, 12))\n    plot!(plts[4]; ylim = (990, 1025))\n    plot(plts...; layout = grid(length(plts), 1), size = (900, 900))\nend\n\nt_grid = collect(range(minimum(t_train), maximum(t_test); length = 500))\ny_pred = predict(y_train[:, 1], t_grid, ps[end], state)\nplot_extrapolation(rescale_t(t_train), rescale_y(y_train), rescale_t(t_test),\n    rescale_y(y_test), rescale_t(t_grid), rescale_y(y_pred))","category":"page"},{"location":"examples/neural_ode_weather_forecast/","page":"Weather forecasting with neural ODEs","title":"Weather forecasting with neural ODEs","text":"While there is some drift in the weather patterns, the model extrapolates very well.","category":"page"},{"location":"#DiffEqFlux:-High-Level-Pre-Built-Architectures-for-Implicit-Deep-Learning","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux: High Level Pre-Built Architectures for Implicit Deep Learning","text":"","category":"section"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"DiffEqFlux.jl is an implicit deep learning library built using the SciML ecosystem. It is a high-level interface that pulls together all the tools with heuristics and helper functions to make training such deep implicit layer models fast and easy.","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"note: Note\nDiffEqFlux.jl is only for pre-built architectures and utility functions for deep implicit learning, mixing differential equations with machine learning. For details on automatic differentiation of equation solvers and adjoint techniques, and using these methods for doing things like calibrating models to data, nonlinear optimal control, and PDE-constrained optimization, see SciMLSensitivity.jl","category":"page"},{"location":"#Pre-Built-Architectures","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"Pre-Built Architectures","text":"","category":"section"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"The approach of this package is the easy and efficient training of Universal Differential Equations. DiffEqFlux.jl provides architectures which match the interfaces of machine learning libraries such as Flux.jl and Lux.jl to make it easy to build continuous-time machine learning layers into larger machine learning applications.","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"The following layer functions exist:","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"Neural Ordinary Differential Equations (Neural ODEs)\nCollocation-Based Neural ODEs (Neural ODEs without a solver, by far the fastest way!)\nMultiple Shooting Neural Ordinary Differential Equations\nNeural Stochastic Differential Equations (Neural SDEs)\nNeural Differential-Algebraic Equations (Neural DAEs)\nNeural Delay Differential Equations (Neural DDEs)\nAugmented Neural ODEs\nHamiltonian Neural Networks (with specialized second order and symplectic integrators)\nContinuous Normalizing Flows (CNF) and FFJORD","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"Examples of how to build architectures from scratch, with tutorials on things like Graph Neural ODEs, can be found in the SciMLSensitivity.jl documentation.","category":"page"},{"location":"#Flux.jl-vs-Lux.jl","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"Flux.jl vs Lux.jl","text":"","category":"section"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"Both Flux and Lux defined neural networks are supported by DiffEqFlux.jl. However, Lux.jl neural networks are greatly preferred for many correctness reasons. Particularly, a Flux Chain does not respect Julia's type promotion rules. This causes major problems in that the restructuring of a Flux neural network will not respect the chosen types from the solver. Demonstration:","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"using Flux, Tracker\n\nx = [0.8; 0.8]\nann = Chain(Dense(2, 10, tanh), Dense(10, 1))\np, re = Flux.destructure(ann)\nz = re(Float64.(p))","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"While one may think this recreates the neural network to act in Float64 precision, it does not and instead its values will silently downgrade everything to Float32. This is only fixed by Chain(Dense(2, 10, tanh), Dense(10, 1)) |> f64. Similar cases will lead to dropped gradients with complex numbers. This is not an issue with the automatic differentiation library commonly associated with Flux (Zygote.jl) but rather due to choices in the neural network library's decision for how to approach type handling and precision. Thus when using DiffEqFlux.jl with Flux, the user must be very careful to ensure that the precision of the arguments are correct, and anything that requires alternative types (like TrackerAdjoint tracked values and ForwardDiffSensitivity dual numbers) are suspect.","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"Lux.jl has none of these issues, is simpler to work with due to the parameters in its function calls being explicit rather than implicit global references, and achieves higher performance. It is built on the same foundations as Flux.jl, such as Zygote and NNLib, and thus it supports the same layers underneath and calls the same kernels. The better performance comes from not having the overhead of restructure required. Thus we highly recommend people use Lux instead and only use the Flux fallbacks for legacy code.","category":"page"},{"location":"#Citation","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"Citation","text":"","category":"section"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"If you use DiffEqFlux.jl or are influenced by its ideas, please cite:","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"@article{rackauckas2020universal,\n  title={Universal differential equations for scientific machine learning},\n  author={Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali},\n  journal={arXiv preprint arXiv:2001.04385},\n  year={2020}\n}","category":"page"},{"location":"#Reproducibility","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"</details>","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"</details>","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"using Pkg # hide\nPkg.status(; mode = PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"</details>","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"using TOML\nusing Markdown\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink_manifest = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n                \"/assets/Manifest.toml\"\nlink_project = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n               \"/assets/Project.toml\"\nMarkdown.parse(\"\"\"You can also download the\n[manifest]($link_manifest)\nfile and the\n[project]($link_project)\nfile.\n\"\"\")","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Convolutional-Neural-ODE-MNIST-Classifier-on-GPU","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"Training a Convolutional Neural Net Classifier for MNIST using a neural ordinary differential equation NN-ODE on GPUs with Minibatching.","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"For a step-by-step tutorial see the tutorial on the MNIST Neural ODE Classification Tutorial using Fully Connected Layers.","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"using DiffEqFlux, Statistics,\n    ComponentArrays, CUDA, Zygote, MLDatasets, OrdinaryDiffEq, Printf, Test, LuxCUDA, Random\nusing Optimization, OptimizationOptimisers\nusing MLDatasets: MNIST\nusing MLDataUtils: LabelEnc, convertlabel, stratifiedobs, batchview\nusing OneHotArrays\n\nconst cdev = cpu_device()\nconst gdev = gpu_device()\n\nCUDA.allowscalar(false)\nENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true\n\nlogitcrossentropy(ŷ, y) = mean(-sum(y .* logsoftmax(ŷ; dims = 1); dims = 1))\n\nfunction loadmnist(batchsize = bs)\n    # Use MLDataUtils LabelEnc for natural onehot conversion\n    function onehot(labels_raw)\n        convertlabel(LabelEnc.OneOfK, labels_raw, LabelEnc.NativeLabels(collect(0:9)))\n    end\n    # Load MNIST\n    mnist = MNIST(; split = :train)\n    imgs, labels_raw = mnist.features, mnist.targets\n    # Process images into (H,W,C,BS) batches\n    x_train = Float32.(reshape(imgs, size(imgs, 1), size(imgs, 2), 1, size(imgs, 3))) |>\n              gdev\n    x_train = batchview(x_train, batchsize)\n    # Onehot and batch the labels\n    y_train = onehot(labels_raw) |> gdev\n    y_train = batchview(y_train, batchsize)\n    return x_train, y_train\nend\n\n# Main\nconst bs = 128\nx_train, y_train = loadmnist(bs)\n\ndown = Chain(Conv((3, 3), 1 => 64, relu; stride = 1), GroupNorm(64, 64),\n    Conv((4, 4), 64 => 64, relu; stride = 2, pad = 1), GroupNorm(64, 64),\n    Conv((4, 4), 64 => 64; stride = 2, pad = 1))\n\ndudt = Chain(Conv((3, 3), 64 => 64, tanh; stride = 1, pad = 1),\n    Conv((3, 3), 64 => 64, tanh; stride = 1, pad = 1))\n\nfc = Chain(GroupNorm(64, 64), x -> relu.(x), MeanPool((6, 6)),\n    x -> reshape(x, (64, :)), Dense(64, 10))\n\nnn_ode = NeuralODE(dudt, (0.0f0, 1.0f0), Tsit5(); save_everystep = false,\n    reltol = 1e-3, abstol = 1e-3, save_start = false)\n\nfunction DiffEqArray_to_Array(x)\n    xarr = gdev(x)\n    return xarr[:, :, :, :, 1]\nend\n\n# Build our over-all model topology\nm = Chain(down,                 # (28, 28, 1, BS) -> (6, 6, 64, BS)\n    nn_ode,               # (6, 6, 64, BS) -> (6, 6, 64, BS, 1)\n    DiffEqArray_to_Array, # (6, 6, 64, BS, 1) -> (6, 6, 64, BS)\n    fc)                   # (6, 6, 64, BS) -> (10, BS)\nps, st = Lux.setup(Random.default_rng(), m)\nps = ComponentArray(ps) |> gdev\nst = st |> gdev\n\n# To understand the intermediate NN-ODE layer, we can examine it's dimensionality\nimg = x_train[1][:, :, :, 1:1] |> gdev\nlab = x_train[2][:, 1:1] |> gdev\n\nx_m, _ = m(img, ps, st)\n\nclassify(x) = argmax.(eachcol(x))\n\nfunction accuracy(model, data, ps, st; n_batches = 10)\n    total_correct = 0\n    total = 0\n    st = Lux.testmode(st)\n    for (x, y) in collect(data)[1:n_batches]\n        target_class = classify(cdev(y))\n        predicted_class = classify(cdev(first(model(x, ps, st))))\n        total_correct += sum(target_class .== predicted_class)\n        total += length(target_class)\n    end\n    return total_correct / total\nend\n\n# burn in accuracy\naccuracy(m, zip(x_train, y_train), ps, st)\n\nfunction loss_function(ps, x, y)\n    pred, st_ = m(x, ps, st)\n    return logitcrossentropy(pred, y), pred\nend\n\n#burn in loss\nloss_function(ps, x_train[1], y_train[1])\n\nopt = OptimizationOptimisers.Adam(0.05)\niter = 0\n\nopt_func = OptimizationFunction((ps, _, x, y) -> loss_function(ps, x, y),\n    Optimization.AutoZygote())\nopt_prob = OptimizationProblem(opt_func, ps)\n\nfunction callback(ps, l, pred)\n    global iter += 1\n    #Monitor that the weights do infact update\n    #Every 10 training iterations show accuracy\n    if (iter % 10 == 0)\n        @info \"[MNIST Conv GPU] Accuracy: $(accuracy(m, zip(x_train, y_train), ps, st))\"\n    end\n    return false\nend\n\n# Train the NN-ODE and monitor the loss and weights.\nres = Optimization.solve(opt_prob, opt, zip(x_train, y_train); maxiters = 10, callback)\n@test accuracy(m, zip(x_train, y_train), res.u, st) > 0.8","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Expected-Output","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Expected Output","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"Iter:   1 || Train Accuracy: 8.453 || Test Accuracy: 8.883\nIter:  11 || Train Accuracy: 14.773 || Test Accuracy: 14.967\nIter:  21 || Train Accuracy: 24.383 || Test Accuracy: 24.433\nIter:  31 || Train Accuracy: 38.820 || Test Accuracy: 38.000\nIter:  41 || Train Accuracy: 30.852 || Test Accuracy: 31.350\nIter:  51 || Train Accuracy: 29.852 || Test Accuracy: 29.433\nIter:  61 || Train Accuracy: 45.195 || Test Accuracy: 45.217\nIter:  71 || Train Accuracy: 70.336 || Test Accuracy: 68.850\nIter:  81 || Train Accuracy: 76.250 || Test Accuracy: 75.783\nIter:  91 || Train Accuracy: 80.867 || Test Accuracy: 81.017\nIter: 101 || Train Accuracy: 86.398 || Test Accuracy: 85.317\nIter: 111 || Train Accuracy: 90.852 || Test Accuracy: 90.650\nIter: 121 || Train Accuracy: 93.477 || Test Accuracy: 92.550\nIter: 131 || Train Accuracy: 93.320 || Test Accuracy: 92.483\nIter: 141 || Train Accuracy: 94.273 || Test Accuracy: 93.567\nIter: 151 || Train Accuracy: 94.531 || Test Accuracy: 93.583\nIter: 161 || Train Accuracy: 94.992 || Test Accuracy: 94.067\nIter: 171 || Train Accuracy: 95.398 || Test Accuracy: 94.883\nIter: 181 || Train Accuracy: 96.945 || Test Accuracy: 95.633\nIter: 191 || Train Accuracy: 96.430 || Test Accuracy: 95.750\nIter: 201 || Train Accuracy: 96.859 || Test Accuracy: 95.983\nIter: 211 || Train Accuracy: 97.359 || Test Accuracy: 96.500\nIter: 221 || Train Accuracy: 96.586 || Test Accuracy: 96.133\nIter: 231 || Train Accuracy: 96.992 || Test Accuracy: 95.833\nIter: 241 || Train Accuracy: 97.148 || Test Accuracy: 95.950\nIter: 251 || Train Accuracy: 96.422 || Test Accuracy: 95.950\nIter: 261 || Train Accuracy: 96.094 || Test Accuracy: 95.633\nIter: 271 || Train Accuracy: 96.719 || Test Accuracy: 95.767\nIter: 281 || Train Accuracy: 96.719 || Test Accuracy: 96.000\nIter: 291 || Train Accuracy: 96.609 || Test Accuracy: 95.817\nIter: 301 || Train Accuracy: 96.656 || Test Accuracy: 96.033\nIter: 311 || Train Accuracy: 97.594 || Test Accuracy: 96.500\nIter: 321 || Train Accuracy: 97.633 || Test Accuracy: 97.083\nIter: 331 || Train Accuracy: 98.008 || Test Accuracy: 97.067\nIter: 341 || Train Accuracy: 98.070 || Test Accuracy: 97.150\nIter: 351 || Train Accuracy: 97.875 || Test Accuracy: 97.050\nIter: 361 || Train Accuracy: 96.922 || Test Accuracy: 96.500\nIter: 371 || Train Accuracy: 97.188 || Test Accuracy: 96.650\nIter: 381 || Train Accuracy: 97.820 || Test Accuracy: 96.783\nIter: 391 || Train Accuracy: 98.156 || Test Accuracy: 97.567\nIter: 401 || Train Accuracy: 98.250 || Test Accuracy: 97.367\nIter: 411 || Train Accuracy: 97.969 || Test Accuracy: 97.267\nIter: 421 || Train Accuracy: 96.555 || Test Accuracy: 95.667","category":"page"},{"location":"layers/TensorLayer/#Tensor-Product-Layer","page":"Tensor Product Layer","title":"Tensor Product Layer","text":"","category":"section"},{"location":"layers/TensorLayer/","page":"Tensor Product Layer","title":"Tensor Product Layer","text":"The following layer is a helper function for easily constructing a TensorLayer, which takes as input an array of n tensor product basis, B_1 B_2  B_n, a data point x, computes zi = Wi  B_1(x1)  B_2(x2)    B_n(xn), where W is the layer's weight, and returns [z[1], ..., z[out]].","category":"page"},{"location":"layers/TensorLayer/","page":"Tensor Product Layer","title":"Tensor Product Layer","text":"TensorLayer","category":"page"},{"location":"layers/TensorLayer/#DiffEqFlux.TensorLayer","page":"Tensor Product Layer","title":"DiffEqFlux.TensorLayer","text":"TensorLayer(model, out_dim::Int, init_p::F = randn) where {F <: Function}\n\nConstructs the Tensor Product Layer, which takes as input an array of n tensor product basis, [B1, B2, ..., Bn] a data point x, computes z[i] = W[i,:] ⨀ [B1(x[1]) ⨂ B2(x[2]) ⨂ ... ⨂ Bn(x[n])], where W is the layer's weight, and returns [z[1], ..., z[out]].\n\nArguments:\n\nmodel: Array of TensorProductBasis [B1(n1), ..., Bk(nk)], where k corresponds to the dimension of the input.\nout: Dimension of the output.\np: Optional initialization of the layer's weight. Initialized to standard normal by default.\n\n\n\n\n\n","category":"function"}]
}
